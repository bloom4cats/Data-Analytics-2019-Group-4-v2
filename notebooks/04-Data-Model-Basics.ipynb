{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 4: Data Modeling Basics\n",
    "\n",
    "## Contents\n",
    "\n",
    "* [Getting Started](#Getting-Started)\n",
    "* [Linear Models](#LinearModels)\n",
    "    * [Concept: Linear Regression](#Concept-LinearRegression)\n",
    "    * [Simple Linear Regression](#SimpleLinearRegression)\n",
    "    * [Multiple Linear Regression](#MultipleLinearRegression)\n",
    "    * [Linear-Like Regression](#Linear-Like-Regression)\n",
    "* [Logistic Regression](#Logistic-Regression)\n",
    "    * [An Example](#An-Example)\n",
    "    * [Home Data](#Home-Data)\n",
    "* [Exercise Answers](#Exercises)\n",
    "* [Next Steps](#Next-Steps)\n",
    "* [Resources and Further Reading](#Resources-and-Further-Reading)\n",
    "* [Notes](#Notes)\n",
    "\n",
    "### Exercises\n",
    "\n",
    "[1](#Exercise-1), [2](#Exercise-2), [3](#Exercise-3), [4](#Exercise-4), [5](#Exercise-5), [6](#Exercise-6), [7](#Exercise-7), [8](#Exercise-8)\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "In previous units, we worked on loading, cleaning, and exploring data.  While working with the data, we noted that certain relationships appeared to exist between columns/variables.  While plots allowed us to make claims like \"x increases as y decreases\",  we didn't try to model that relationship mathematically nor did we try to determine the quality of that model.  \n",
    "\n",
    "In this unit,we'll look at creating models for the relationships in our data; specifically, we'll look at linear models for numerical data and logistic models for categorical data.\n",
    "\n",
    "To create and explore these models, we'll use the [StatsModels](https://www.statsmodels.org/stable/index.html) library. To install it, we'll use `!pip`.\n",
    "\n",
    "If using [Anaconda](https://www.anaconda.com/download) and the following pip command fails, open the Anaconda prompt on your computer and run the following\n",
    "\n",
    "```\n",
    "conda install --yes statsmodels\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll work with plots in this unit.  To ensure they are displayed in the notebook itself, we'll need to use the `%matplotlib inline` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the Seaborn and pandas libraries, we'll make explicit use of libraries on which they depend.\n",
    "\n",
    "- [matplotlib.pyplot](https://matplotlib.org/api/pyplot_api.html): a collection of plotting functions with [MATLAB](https://www.mathworks.com/products/matlab.html)-like syntax\n",
    "- [numpy](http://www.numpy.org/): scientific computing library\n",
    "\n",
    "We'll import these and StatsModels with names that follow standard convention.\n",
    "\n",
    "We also set the figure size for plots and a marker size for outliers in box plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "sns.set(rc={'figure.figsize':(10,10), \"lines.markeredgewidth\": 0.5 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"LinearModels\"></a>\n",
    "## Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<a name=\"Concept-LinearRegression\"></a>\n",
    "#### Concept: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example of how we can calculate the coefficients of a linear equation that models some data. We'll start with an example based on the [StatsModels documentation](http://www.statsmodels.org/stable/examples/notebooks/generated/ols.html).\n",
    "\n",
    "First, we generate our \"observed\" data.  To do this, we'll use the Numpy [*linspace()*](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html) function to generate 100 evenly-spaced values between 0 and 10; these will correspond to values that will be used for the independent variable.  Next, we'll use the [*normal()*](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html) function from NumPy's [*random*](https://docs.scipy.org/doc/numpy-1.14.0/reference/routines.random.html) submodule to draw 100 samples from a normal distribution - this will simulate errors in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsample = 100\n",
    "x = np.linspace(0, 10, nsample)\n",
    "e = np.random.normal(size=nsample)\n",
    "\n",
    "display(x[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first 10 values of `x`, we can see that they are stored in an [array](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.array.html).\n",
    "\n",
    "Next, we calculate our \"observed\" values as a combination of the independent variable, a constant, and some error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 5 * x + 3 + e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the values of `x` and `y`.  We'll create a scatter plot but use a different approach to do this.  \n",
    "\n",
    "First, we create [figure](https://matplotlib.org/api/_as_gen/matplotlib.figure.Figure.html#matplotlib.figure.Figure) and [axes](https://matplotlib.org/api/axes_api.html#matplotlib.axes.Axes) objects using the pyplot [*subplots()*](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html) function; this is useful when we want to plot items from different sources together (as we will do in a bit). We use the axes' [*plot()*](https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.plot.html#matplotlib.axes.Axes.plot) method to plot the coordinate pairs from `x` and `y`; the third argument indicates that we'd like to use blue plus-sign markers rather than draw lines from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, 'b+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown by the plot, the data does look linearly related.  One measure of the strength of the relationship is the [correlation coefficient](https://en.wikipedia.org/wiki/Correlation_coefficient).  Given two variables, the correlation coefficent can have a value between -1 and 1 where -1 indicates strong, negative relationship (as one variable increases, the other decreases) and 1 indicates a strong, postive relationship (as one variable increases, the other increases); a valeu of zero indicates no relationship exists.\n",
    "\n",
    "To calculate the correlation coefficient, we can use the NumPy [*corrcoef()*](https://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is the correlation coefficient matrix that shows from left to right, top to bottom, correlation cofficient between the first variable and itself, the correlation cofficient between the first variable and the second, the correlation cofficient between the second variable and the first, and the correlation cofficient between the second variable and itself.  We expect that a variable will be strongly correlated with itself.  The output indicates a strong relationship between the variables.  By construction, the relationship is not only strong, it is also very linear.  To see this, we can use a linear regression.\n",
    "\n",
    "In the two-dimensional linear regression. we need to calculate the values of two coefficients: a constant and a value that will be multiplied by the value of the independent variable.  As we saw above, we can write the linear model in the form\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "Y = \\beta_0 + \\beta_1 X\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We can write the constant, $\\beta_0$ as $\\beta_0 X^0$  Since any value raised to the zeroth power is one, we can write $\\beta_0$ as $1 \\cdot \\beta_0$.  We can say that the dependent variable is a linear combination of 1 and the independent variable.  For our model, we account for this by using the StatsModels' [*add_constant()*](http://www.statsmodels.org/dev/generated/statsmodels.tools.tools.add_constant.html) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(x)\n",
    "display(X[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing `X` to `x` we now have an array of arrays where the first element in the inner arrays are `1` corresponding to the value of the independent variable for the constant term. The corresponds to the matrix $\\mathbf{X}$ described above.\n",
    "\n",
    "To compute the the linear regression, we first set up the ordinary least squares model using the StatsModels' [*OLS*](http://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html) function and by specifying the array of values for the dependent variable and the array of values for the independent variable.  \n",
    "\n",
    "With the model created, we calculate the regression coefficients using the model's [*fit()*](http://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.fit.html#statsmodels.regression.linear_model.OLS.fit) method; this method returns a [*RegressionResults*](http://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.RegressionResults.html) object. To dispaly information about the fit, we display the output of the result's [*summary()*](http://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.RegressionResults.summary.html#statsmodels.regression.linear_model.RegressionResults.summary) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "display(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, the number of observations, the coefficients, their p-values, and the value of `R-squared` are particularly of interest.  We can access these directly from `results` using the *nobs*, *params*, *pvalues*, and *rsquared* properties as well. (See StatsModels [RegressionResults](http://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.RegressionResults.html#statsmodels.regression.linear_model.RegressionResults) for more options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No. of Obs.: ', results.nobs)\n",
    "print('Parameters: ', results.params)\n",
    "print(\"P-values:\", results.pvalues)\n",
    "print('R^2: ', results.rsquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients appear in the same order in which the independent variables appear in `X`.\n",
    "\n",
    "Note that this is \"close\" to the equation we used to generate the data - the discrepancy is due to the error we introduced.  \n",
    "\n",
    "Let's plot the regression line along with our data.  We can repeat the same steps as before to create the scatter plot.  We make an additional call to *plot()* and specify the y-values as the output from the results *predict()* method which returns the predicted values of the dependent variable based on the regression results.  Specifying `'r'` for the third argument to plot indicates that we would like the line to be red.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.plot(x, y, 'b+')\n",
    "ax.plot(x, results.predict(), 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [p-value](https://en.wikipedia.org/wiki/P-value) associated with each coefficient indicates how likely changes to the corresponding independent variable account for changes in the dependent variable.  A p-value close to zero (typically, less than 0.05) indicates that the independent variable provides a meaningful addition to the model. <sup><a href=\"#note-1\">1</a></sup>\n",
    "\n",
    "The `R-squared` value is also known as the [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination) and provides a measure of how well the regression line fits the data.  The coefficient of determination can range from 0 to 1 with 0 indicating (in some regressions, the value can be negative) how much of the variation in the dependent variable can be explained by the model - a value of 1 indicates that all variation is explained by the model and 0 indicates that the model accounts for none of the variation. In this example, some of the variation is due to the error we introduced - changing the magnitude of the error or the parameters of the distribution from which values were drawn will result in a better or worse coefficient of determination.\n",
    "\n",
    "Assuming the fit is a good one, we can use it to predict other values not in the initial data. The simplest way to do this for individual values is by defining a function that takes an input value as a parameter and computes and returns the output based on the calculate coefficients. (In the example below, we place a `'_'` character after the function name, `predict_` to keep it distinct from the `predict` method provided by the statsmodels package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_(model_results, value):\n",
    "    const, coeff = model_results.params\n",
    "    return const + coeff * value\n",
    "\n",
    "print(predict_(results, 4.5))\n",
    "print(predict_(results, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<a name=\"SimpleLinearRegression\"></a>\n",
    "### Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example based on real data. To begin, we'll reload the county auditor data we processed previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///../data/output.sqlite')\n",
    "\n",
    "query = \"SELECT * from pit_df;\"\n",
    "df = pd.read_sql_query(query, con=engine)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that in Unit 3 we had more reasonable distributions of sale prices when we did some filtering. Let's load the \"sampling.py\" file, and use the included function `unit3_df_redux` to repeat the data modifications we made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src/cscc_ada\"))\n",
    "from sampling import *\n",
    "\n",
    "df_sub = unit3_df_redux(df)\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's look at a the `yearbuilt`, `appraisedtaxablebuilding`, `saleprice`, and `area` columns in `df_sub`. First we can calculate the correleation coefficient matrix. With a DataFrame, we can use the *corr()* method to calculate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_sub[[\"yearbuilt\", \"appraisedtaxablebuilding\", \"saleprice\", \"area\", \"bathrooms\", \"bedrooms\", \"acreage\"]].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No change, so although removing those rows didn't improve our results, it didn't negatively impact them either, which is good. To get sense of these relationships visually we can use a pair plot, just like what we did in Unit 3.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<a name=\"Exercise-1\"></a><mark> **Exercise 1** In the cell below, create a pair plot for the  `yearbuilt`, `appraisedtaxablebuilding`, `saleprice`, `area`, `bathrooms`, `bedrooms`, and `acreage` columns in `df_avg`.\n",
    "</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by  look more closely at the most correlated relationship we found which was between `area` and `bathrooms`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<a name=\"Exercise-2\"></a><mark> **Exercise 2** In the cell below, create scatter plot for the `area` and `bathrooms` columns in `df_avg`.\n",
    "</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we see that there are dwellings with areas of zero and zero bathrooms. These dwellings are unimportant for our analysis, so let's drop them and create a new subset to work with further. Additionally, let's not consider any dwelling with a sale price or appraised value above 500k dollars because there is too much noise in those properties to be useful for our simple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg = df_sub[df_sub.area > 0]\n",
    "df_avg = df_avg[df_avg.bathrooms > 0]\n",
    "df_avg = df_avg[df_avg.saleprice < 500000]\n",
    "df_avg = df_avg[df_avg.appraisedtaxablebuilding < 500000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's review the pair plots again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_avg[columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Next, lets create the least squares model and fit a line to the data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(df_avg.appraisedtaxablebuilding)\n",
    "Y = df_avg.saleprice\n",
    "model = sm.OLS(Y, X)\n",
    "res = model.fit()\n",
    "display(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `R-squared` value we can see there is a moderate linear relationship between the data; further, the p-value for the coefficient of `saleprice` indicates that changes in `displ` are likely attributed to changes in `cylinders`.  \n",
    "\n",
    "To plot the fit line with the scatter plot generated by the DataFrame's *plot()* method we can use StatsModel's [*abline_plot()*](http://www.statsmodels.org/dev/generated/statsmodels.graphics.regressionplots.abline_plot.html) function.  First, we import the function then create a scatter plot and store the returned *axes* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.regressionplots import abline_plot\n",
    "axes = df_avg.plot.scatter(x=\"appraisedtaxablebuilding\", y=\"saleprice\", c='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can can add the plot of the regression line to the plot using *abline_plot()* function, specifying the model results, the *axes*, and a color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abline_plot(model_results=res, ax=axes, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart above shows just how many outliers we have in our data. There are likely multiple reasons why dwellings sell for much more than they are appraised for (the left-hand side of the chart), as well as reasons for selling for far less than they are appraised for (bottom of the chart). A perfect correlation would be a 45-degree diagonal line as we saw in our example code above. With real-world data such a result is rare. For the purpose of illustrating more linear regression techniques, we are going to \"cheat\" and temporarily remove most of the outliers from our data. We can also use Seaborn as we did in Unit 3, and allow the `lmplot()` function to perform automatic regression fits for us by category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_mod = df_avg.query(\"not (saleprice / appraisedtaxablebuilding > 3.5) \"\n",
    "                             \"and not (appraisedtaxablebuilding / saleprice > 3.5)\")\n",
    "sns.lmplot(x='appraisedtaxablebuilding', y=\"saleprice\",\n",
    "           height=10, hue='condition', \n",
    "           data=df_avg_mod, fit_reg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we arbitrarily remove all of those outliers, we can see that our fit lines do begin to match the general trend that is represented by all of the dots. Let's re-run our regression and then plot against our original regression data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = sm.add_constant(df_avg_mod.appraisedtaxablebuilding)\n",
    "Y2 = df_avg_mod.saleprice\n",
    "model2 = sm.OLS(Y2, X2)\n",
    "res2 = model2.fit()\n",
    "display(res2.summary())\n",
    "axes2 = df_avg_mod.plot.scatter(x=\"appraisedtaxablebuilding\", y=\"saleprice\", c='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our `R-squared` value improved from 0.526 to 0.733, which means that our prediction accuracy is better. Let's look at it graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abline_plot(model_results=res, ax=axes, color='b')\n",
    "abline_plot(model_results=res2, ax=axes, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original regression line is colored red and the regression line calculated with outliers removed is colored red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By removing so many outliers, what types of assumptions might we be making? If we remove all of those datapoints, how might we be over-simplifying our model? These are just some of the questions we would want to resolve before declaring our linear regression model complete and useful for making inferrances and predictions. However, for this lesson we are primarily trying to learn how to perform a linear regression and refine our results, not to build a fully functional housing prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"MultipleLinearRegression\"></a>\n",
    "### Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've looked at regressions in which there is one independent variable and a constant.  Often, changes in the response variable are dependent on multiple variables. For example. we expect that the sale price, `saleprice`, is dependent on both appraised value, `appraisedtaxablebuilding`, and the size of the dwelling, `area`. We can see from the scatter plots that `comb08` looks linearly dependent on `city08` and `highway08`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_mod.plot.scatter(x='appraisedtaxablebuilding', y='saleprice')\n",
    "df_avg_mod.plot.scatter(x='area', y='saleprice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also fit models for each of these individually - `appraisedtaxablebuilding`/`saleprice` and `area`/`saleprice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(df_avg_mod.appraisedtaxablebuilding)\n",
    "Y = df_avg_mod.saleprice\n",
    "model = sm.OLS(Y, X)\n",
    "res = model.fit()\n",
    "display(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(df_avg_mod.area)\n",
    "Y = df_avg_mod.saleprice\n",
    "model = sm.OLS(Y, X)\n",
    "res = model.fit()\n",
    "display(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separately, the models fit the data well enough. Let's look at the model in which both `appraisedtaxablebuilding` and `area` are independent variables used to predict `saleprice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(df_avg_mod[['area','appraisedtaxablebuilding']])\n",
    "Y = df_avg_mod.saleprice\n",
    "model = sm.OLS(Y, X)\n",
    "res = model.fit()\n",
    "display(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the coefficient of determination, we can see this model, which depends on both `appraisedtaxablebuilding` and `area`, fits the data a bit better than a model which depends on only one of the variables.\n",
    "\n",
    "We can predict values using the results in a method similar to before using a a custom function that accepts values for each of the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_results, area, appraisedtaxablebuilding):\n",
    "    const, area_coeff, appraisedtaxablebuilding_coeff = model_results.params\n",
    "    return const + area_coeff * area + appraisedtaxablebuilding_coeff * appraisedtaxablebuilding\n",
    "\n",
    "print(predict(res, 2500, 150000))\n",
    "print(predict(res, 4500, 300000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our problem, the results indicate that if a dwellings's area is 2500 sq. ft. and the appraised building value is 150k dollars, the sale price will be about 167,500 dollars. Similarly, if the sq. ft. is 4500 and appraised value is 300000, respectively, the sale price is predicted to be about 321,900 dollars. However, we do need to remember that this simple regression (which does exclude many of the properties in our full dataset) is not particularly accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Linear-Like-Regression\"></a>\n",
    "### Linear-Like Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are many of relationships that are linear and that can be modeled using a linear regression, there are also relationships that are non-linear.  Among these non-linear relationships are those that can transformed into a linear ones in terms of the coefficients and independent variables.\n",
    "\n",
    "As an example, consider the relationship between `comb08` and `c02`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_mod2 = df_avg_mod[df_avg_mod.area < 10000]\n",
    "df_avg_mod2 = df_avg_mod2[df_avg_mod2.area > 200]\n",
    "df_avg_mod2.plot.scatter(x=\"yearbuilt\", y=\"area\", c='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this relationship doesn't appear to be linear, it does look [hyperbolic](https://en.wikipedia.org/wiki/Hyperbola). In this case, the relationship between the independent variable and dependent variable could be written as \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "Y = \\frac{1}{\\beta_0 + \\beta_1 X}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "To move the coefficients and independent variable out of the denominator, we can take the reciprocal of both sides (provided the neither side is zero) - this gives us\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\frac{1}{Y} = \\beta_0 + \\beta_1 X\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "For a given observation, this form is easier to work with since $\\frac{1}{Y}$ and $X$ are constants and we need to solve for $\\beta_0$ and $\\beta_1$. \n",
    "\n",
    "We can calculate the reciprocal of the dependent variable, `co2`, and store the value in a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_non_linear = df_avg_mod2[['yearbuilt', 'area']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform the problem into a linear one, we need to calculate the reciprocal of the `co2` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_non_linear[\"reciprocal_area\"] = 1 / df_avg_non_linear.area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot of `yearbuilt` and `area`, it appears that the relationship is linear, which supports our assumption that the original relationship was hyperbolic.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<a name=\"Exercise-5\"></a><mark> **Exercise 5** In the cell below, create a scatter plot of `yearbuilt` and `reciprocal_area`.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "Before fitting the data with a linear model, let's remove the outliers.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<a name=\"Exercise-6\"></a><mark> **Exercise 6** In the cell below, remove the outliers from the `comb08` and `reciprocal_co2` columns in the `epa_non_linear` DataFrame. Use the *remove_outliers()* function we created earlier.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "With the outliers removed, let's look at the scatter plot again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = df_avg_non_linear.plot.scatter(x=\"yearbuilt\", y=\"reciprocal_area\", c='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a linear model for `comb08` and `reciprocal_c02`.  After calculating the coefficients, we display the summary of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(df_avg_non_linear[\"area\"])\n",
    "Y = df_avg_non_linear[\"reciprocal_area\"]\n",
    "model = sm.OLS(Y, X)\n",
    "res = model.fit()\n",
    "display(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the coefficient of determination, we see that the model produced a good fit. Adding the regression line to the existing scatter plot give the following plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abline_plot(model_results=res, ax=axes, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to transform the regression line to fit the original data.  Using the *params* property of the results we have the following constant term and coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that our model is \n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "Y = \\frac{1}{0.000068 + 0.000109 X}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We can calculate the predicted values from our model using the following code.  We use *sort_values()* to ensure that the values of the independent variable are in order. This will be important when we plot the curve; we didn't need to do this previously as we relied on the *abline_plot()* function, which handled this for us,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = 1/(res.params.const + res.params.area * df_avg_mod2.yearbuilt.sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the model curve against our original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(10,10))\n",
    "axes.plot(df_avg_mod2.yearbuilt, df_avg_mod2.area, 'b+')\n",
    "axes.plot(df_avg_mod2.yearbuilt.sort_values(), prediction, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Logistic-Regression\"></a>\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) is used to model data where the dependent variable is categorical. In the simplest case, the dependent variable is binary and has only two possible values. A logistic model, provides an estimate of the probability that one of the two categories applies given the values of the independent variables; it fits a [logistic probability distribution](https://en.wikipedia.org/wiki/Logistic_distribution) to the data. We'll only look at simple case where the dependent variable is binary and there is only one independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, consider the the following [example taken from the Wikipedia page on logistic regressions](https://en.wikipedia.org/wiki/Logistic_regression#Example:_Probability_of_passing_an_exam_versus_hours_of_study).  We have two variables: *hours* and *passed*.  The *hours* variable represents the number of hours a student spent studying for an exam and *passed* indicates whether or not the student passed the exam where `0` indicates failure and `1` indicates that the student passed; *hours* is a continuous variable and *passed* is a discrete, binary variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours = [0.5, 0.75, 1.0, 1.25, 1.50, 1.75, 1.75, 2.0, 2.25, 2.5, 2.75, 3.0, 3.25, 3.5, 4.0, 4.25, 4.5, 4.75, 5.0, 5.5]\n",
    "passed = [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting this data as a scatter plot give the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hours, passed, 'b+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression calculates values for $\\beta_0$ and $\\beta_1$ in the following formula\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "P = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $P$ is is a probability value between 0 and 1 and $X$ is the independent variable.\n",
    "\n",
    "We can use the StatsModels [*Logit()*](http://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.Logit.html) function to perform the logistic regression.  \n",
    "\n",
    "We have to reassign the value of `stats.chisqprob` due to a discrepancy between the StatsModels module and the libraries on which it depends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\n",
    "\n",
    "X = sm.add_constant(hours)\n",
    "logit_model=sm.Logit(passed,X)\n",
    "result=logit_model.fit()\n",
    "display(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the model in much the same way as we did for a linear model.  We specify the independent variable and add a constant using StatsModels' *add_constant()* function.  We next create a logistic model using the *Logit()* function.  To calculate the coefficients, we use model's *fit()* method.  After the calculation is complete, we can view a summary of the results.\n",
    "\n",
    "Just like the linear model, the results of the logistic model have a *predict()* method that give the model's predicted values based on the values of the independent variable.  We can use this to plot the logistic curve along with the scatter plot of the data. \n",
    "\n",
    "The model represents the probability of passing the exam given some number of hours spent studying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(10,8))\n",
    "axes.plot(hours,passed, 'b+')\n",
    "axes.plot(hours, result.predict(), 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the probability of for a given value of the independent variable using a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(model_results, value):\n",
    "    const, coeff = model_results.params\n",
    "    exponent = -(const + coeff * value)\n",
    "    denominator = 1 + np.exp(exponent)\n",
    "    return 1 / denominator\n",
    "\n",
    "print(probability(result, 2))\n",
    "print(probability(result, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, the results indicate if a student spends 2 hours studying there is about a 26% likelihood that the student will pass the exam; similarly if the student spends 6 hours studying, the probability of passing is greater than 99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an example with real data, consider the count auditor data we worked with previously.  We can load the data from our local database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, lets see if we can calculate the logistic model that gives the probability of a house having a fireplace given its area.  Currently, the dataset includes the the number of fireplaces a given property has; we need to convert values greater than zero to 1, indicating that there is a fireplace.  \n",
    "\n",
    "First, we drop any rows with missing data. Next, we create a new column, `HasFireplace` that is equal to the mask corresponding the `Fireplaces` being greater than zero. Masks return values of `True` or `False` and the *astype(int)* function call will convert the boolean value to an integer where `False` becomes 0 and `True` becomes 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg['fireplacesflag'] = (df_avg.fireplacesflag > 0).astype(int)\n",
    "df_avg.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the scatter plot of `Area` and `HasFireplaces`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_avg_mod2.plot.scatter(x=\"area\", y=\"fireplacesflag\", c='b')\n",
    "plt.plot(df_avg_mod2.area, df_avg_mod2.fireplacesflag, 'b+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating the model, we sort the values of our independent variable; this will aid in plotting later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_mod2.sort_values(by=[\"area\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the logistic model and calculating the regression coefficients is similar to the logistic model as we noted in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "X = sm.add_constant(df_avg_mod2.area)\n",
    "logit_model=sm.Logit(df_avg_mod2.fireplacesflag,X)\n",
    "result=logit_model.fit()\n",
    "display(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model created and the coefficients calculated, we can now plot the regression curve against the original data.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<a name=\"Exercise-7\"></a><mark> **Exercise 7** In the cell below, create a scatter plot of the `area` and `fireplacesflag` columns from the `df_avg_mod2` DataFrame. On the same figure, plot the logistic regression curve.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "The formula for the curve is given by\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "P = \\frac{1}{1 + e^{-(-1.7215 + 0.0008 X)}}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "For a given area, we can calculate the probability that the house has a fireplace using this formula based on the model.\n",
    "\n",
    "Let's look at one more example.  Older home tend to have fewer bathrooms. We can create a new column, `MoreThanOneBathroom`\n",
    "\n",
    "<hr>\n",
    "\n",
    "<a name=\"Exercise-8\"></a><mark> **Exercise 8** In the cell below, create a new column named `MoreThanOneBathroom` in the `df_avg` DataFrame that has a value of 0 if the value of `bathrooms` is less than or equal to 1 and has a value of 1 otherwise.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "We should sort the data by `yearbuilt` before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg.sort_values(by=[\"yearbuilt\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg.MoreThanOneBathroom.isna().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we construct the model with `yearbuilt` as the independent variable and `MoreThanOneBathroom` as the dependent variable. We calculate a logistic curve that best fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(df_avg.yearbuilt)\n",
    "logit_model=sm.Logit(df_avg.MoreThanOneBathroom, X)\n",
    "result=logit_model.fit()\n",
    "display(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create a scatter plot of `yearbuilt` and `MoreThanOneBathroom` along with the logistic regression curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(10,8))\n",
    "axes.plot(df_avg.yearbuilt, df_avg.MoreThanOneBathroom, 'b+')\n",
    "axes.plot(df_avg.yearbuilt, result.predict(), 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Answers\n",
    "\n",
    "1. ```python\n",
    "   columns = [ \"yearbuilt\", \"appraisedtaxablebuilding\", \"saleprice\", \"area\", \"bathrooms\", \"bedrooms\", \"acreage\" ]\n",
    "   sns.pairplot(df_avg[columns])\n",
    "   ```\n",
    "   \n",
    "   \n",
    "2. ```python\n",
    "   df_avg.plot.scatter(x=\"area\", y=\"bathrooms\", c='b')\n",
    "   ```\n",
    "   \n",
    "   \n",
    "4. ```python\n",
    "   X = sm.add_constant(epa_no_outliers.cylinders)\n",
    "   Y = epa_no_outliers.displ\n",
    "   model = sm.OLS(Y, X)\n",
    "   res_no_outliers = model.fit()\n",
    "   ```\n",
    "   \n",
    "   \n",
    "5. ```python\n",
    "   df_avg_non_linear.plot.scatter(x=\"yearbuilt\", y=\"reciprocal_area\", c='b')\n",
    "   ```\n",
    "   \n",
    "   \n",
    "6. ```python\n",
    "   def remove_outliers(dataframe, column):\n",
    "   q1 = dataframe[column].quantile(0.25)\n",
    "   q3 = dataframe[column].quantile(0.75)\n",
    "   iqr = q3 - q1\n",
    "   lower = q1 - 1.5 * iqr\n",
    "   upper = q3 + 1.5 * iqr\n",
    "   return dataframe[(dataframe[column] >= lower) &\n",
    "                    (dataframe[column] <= upper)].copy()\n",
    "   df_avg_non_linear = remove_outliers(df_avg_non_linear, \"yearbuilt\")\n",
    "   df_avg_non_linear = remove_outliers(df_avg_non_linear, \"reciprocal_area\")\n",
    "   ```\n",
    "   \n",
    "   \n",
    "7. ```python\n",
    "   import matplotlib.pyplot as plt\n",
    "   fig, axes = plt.subplots(figsize=(10,8))\n",
    "   axes.plot(df_avg_mod2.area,df_avg_mod2.fireplacesflag, 'b+')\n",
    "   axes.plot(df_avg_mod2.area, result.predict(), 'r-')\n",
    "   ```\n",
    "\n",
    "\n",
    "8. ```python\n",
    "   df_avg[\"MoreThanOneBathroom\"] = (df_avg.bathrooms > 1).astype(int)\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models we create can be used populate dashboards or included in reports. Later, we'll look at creating visualizations and reporting information. Model creation could also be an intermediate step in the analysis process; in a later unit we'll look at automating model creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources and Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Simple and Multiple Linear Regression in Python](https://towardsdatascience.com/simple-and-multiple-linear-regression-in-python-c928425168f9)\n",
    "- [Logistic Regression in Python Using Rodeo](http://blog.yhat.com/posts/logistic-regression-python-rodeo.html)\n",
    "- [Regression Analysis with Python by Massaron and Boschetti (Safari Books)](http://proquest.safaribooksonline.com.cscc.ohionet.org/book/programming/python/9781785286315)\n",
    "- [Data Science Algorithms in a Week by Natingga, Regression (Safari Books)](http://proquest.safaribooksonline.com.cscc.ohionet.org/book/programming/machine-learning/9781787284586/regression/1500cb6b_9703_4b4a_bffb_61da8fbd2e97_xhtml?uicode=ohlink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <a name=\"note-1\"></a> The null hypothesis being tested is that the variable associated with the coefficient has no effect on the dependent variable.  When the p-value is sufficiently low, typically less the 0.05, we reject the null hypothesis thereby accepting that the variable does have an effect on the dependent variable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
