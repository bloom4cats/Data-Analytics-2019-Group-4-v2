{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Top\"></a>\n",
    "\n",
    "# Unit 2: Data Preparation\n",
    "\n",
    "## Contents\n",
    "\n",
    "* [Getting Started](#Getting-Started)\n",
    "* Data Preparation\n",
    "    * [Example 1: Franklin County Audit Data](#Example-1:-Franklin-County-Audit-Data)\n",
    "        * [Sample Data](#1-SampleData)\n",
    "        * [Clean Data](#1-CleanData)\n",
    "        * [Construct Data](#1-ConstructData)\n",
    "        * [Integrate Data](#1-IntegrateData)\n",
    "    * [Example 2: Parsing Errors with Licking County Auditor Data](#Example-2:-Parsing-Errors-with-Licking-County-Auditor-Data)\n",
    "        * [Concept: Append Data](#Concept-Append)\n",
    "    * [Extraction from a GIS Dataset: Fairfield County Auditor Data](#GIS-dataset-extraction)\n",
    "        * [Concept: Data Manipulation](#Concept-DataManip)\n",
    "* [Joining Related Datasets](#Joining-Related-Datasets)\n",
    "* [Lab Answers](#Lab-Answers)\n",
    "* [Next Steps](#Next-Steps)\n",
    "* [Resources and Further Reading](#Resources-and-Further-Reading)\n",
    "\n",
    "### Exercises\n",
    "\n",
    "[1](#Exercise-1), [2](#Exercise-2), [3](#Exercise-3), [4](#Exercise-4), [5](#Exercise-5), [6](#Exercise-6), [7](#Exercise-7), [8](#Exercise-8), [9](#Exercise-9),  [10](#Exercise-10), [11](#Exercise-11), [12](#Exercise-12), [13](#Exercise-13), [14](#Exercise-14), [15](#Exercise-15)\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "In addition to libraries we used in the last unit, this notebook relies on the [GeoPandas](library) to process data from a [geographic information system](https://en.wikipedia.org/wiki/Geographic_information_system). To install the library, we should use `pip` since the Anaconda package manager, `conda`, does not install one of the underlying support modules for `geopandas` the same way that `pip` does.\n",
    "\n",
    "NOTE: If using [Anaconda](https://www.anaconda.com/download) and the following pip command fails, try installing `geopandas` with the Anaconda prompt on your computer by running the following command:\n",
    "\n",
    "```\n",
    "conda install --yes geopandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Example-1:-Franklin-County-Audit-Data\"></a>\n",
    "### Example 1: Franklin County Auditor Data\n",
    "We'll be working with county auditor data containing real estate information. The full dataset is quite large - 400,000+ rows - so we need to sample the data for initial data cleansing and construction for later combination with other datasets from other central Ohio counties. We will only be looking at 10% of the entire dataset, which is a somewhat arbitrary data sampling approach, but assuming we have no idea what might interest us with this dataset it's a good place to start.\n",
    "\n",
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1-SampleData\"></a>\n",
    "### 1. Sample Data\n",
    "To reduce the memory and time required to process the Franklin County data, the size of the dataset will be reduced; the `sample_file()` function below contains the code to do this. Two arguments are required when the function is called, `input_file` and `output_file`, to specify the source and target files, respectively. A keyword argument, `fraction`, can be specified to set the desired size of the output file relative to the input file; the default value is 0.1.\n",
    "\n",
    "First, `sample_file()` calls `get_line_count()` to calculate the total number of lines in the source file. The total is multiplied by the specified fraction and truncated to the nearest integer to determine the number of lines that should be in the output file. Next, the `sample()` function from the random module is used to select a sampling of line numbers from a range of values starting at 1 and ending at the last line number in the source file; the number of line numbers in the sample is equal to the calculated sample line count. Finally, the function iterates through the source file, line-by-line, and copies a line to the output file if that line's line number is in the sample of line numbers.\n",
    "\n",
    "**NOTE:** We are using the very simplistic `random.sample()` function which is good enough for our learning purposes in this lesson. We also initialize the `seed()` used to determine which rows we grab \"randomly\" from our dataset. This allows some consistency in our lesson, but is not necessary in real-world practice. As you become more proficient with Python you will find that there are more robust data sampling functions and methods available which you will want to use for more statistically defensible work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import linecache\n",
    "import random\n",
    "\n",
    "def get_line_count(input_file):\n",
    "    \"\"\"Count number of lines in a file\"\"\"\n",
    "    count = 0\n",
    "    with open(input_file) as infile:\n",
    "        for line in infile:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def sample_file(input_file, output_file, fraction=0.1):\n",
    "    \"\"\"Exctract a subset of lines from a file\"\"\"\n",
    "    total_line_count = get_line_count(input_file)\n",
    "    sample_line_count = int(fraction * total_line_count)  # fraction of total\n",
    "    random.seed(12345)  # set an arbitrary number to force random.sample() to return same same sample of rows each time we call sample_file method\n",
    "    sample_line_numbers = random.sample(range(1, total_line_count), \n",
    "                                        sample_line_count)  # sample of line numbers\n",
    "    sample_line_numbers.sort()\n",
    "    sample_line_numbers.insert(0, 0)\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for line_number in sample_line_numbers:\n",
    "            line = linecache.getline(input_file, line_number + 1)\n",
    "            outfile.write(line)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a data sampling function, let's use it with Franklin County data. The Franklin County Auditor's website offers the ability to [generate datasets](https://apps.franklincountyauditor.com/reporter). Another approach is to download the [data files via FTP](ftp://apps.franklincountyauditor.com/), which is the approach we'll use here. \n",
    "\n",
    "We will first load the data file, then sample it using the `sample_file()` function that we defined above, and finally we will store the sampled output as a data file which we can use again in further steps.\n",
    "\n",
    "The `pandas` package can be used to read the franklin county data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load full dataset for initial analysis\n",
    "franklin = pd.read_csv('../data/county_auditor/OH-Franklin/Parcel.csv')\n",
    "\n",
    "# sample the data (default is 10% of all rows)\n",
    "sample_file('../data/county_auditor/OH-Franklin/Parcel.csv', \n",
    "            '../data/county_auditor/OH-Franklin/franklin_auditor_subset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1-CleanData\"></a>\n",
    "### 2. Clean Data\n",
    "In addition to simply extracting the data, we often need to address data quality issues as well; examples of quality issues include\n",
    "\n",
    "- *duplication*: the dataset unnecessarily includes repeated data\n",
    "- *inconsistency*: different values are used to represent the same thing or the values do not fit the defined schema\n",
    "- *incompleteness*: data is missing from the dataset\n",
    "- *inaccuracy*: the data does not reflect what it purports to measure or represent\n",
    "\n",
    "Resolving duplication issues is usually straightforward: duplicate data is removed before conducting further analysis. Inconsistencies can be resolved by determining the appropriate values and transforming the data as needed; however, realizing that multiple values correspond to the same thing might require some examination of the data.  While it can be easy to detect incompleteness of data, the approach for resolving the issue might be context-specific. Should a default value be used? Should a randomly generated value within some range be substituted? Should records with missing data be dropped entirely? Inaccuracies tend to be more difficult to detect and to resolve as they often require examining how the source data was collected.\n",
    "\n",
    "Let's begin by looking at the original documentation about the Franklin County dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display auditor documentation in the notebook (NOTE: May not work properly in all browsers)\n",
    "from IPython.display import IFrame\n",
    "IFrame(src=\"../data/county_auditor/OH-Franklin/documentation/ParcelDataReadme.pdf\", height=800, width=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following examples, we'll work on aggregating data from three different county auditor datasets. Our immediate objective is to collect any data regarding appraisal values, sale price, total area, the number of rooms, information about heating and cooling, and the year built for residential properties.\n",
    "\n",
    "With the data loaded, we can now begin examining it. To start, we can see the complete list of columns in the dataset.  Compare this to the columns in the documentation - there are column names in the dataset that do not appear in the documentation and column names in the documentation that do not appear in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "franklin.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the first few rows using the DataFrame's *head()* method.  We'll increase the number of columns displayed to 200 to accommodate the datasets we'll be working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_column', 200)\n",
    "franklin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining these rows, we can get a sense of the type of data in each column.  We can also see that some values are `NaN` which stands for \"Not a Number\" and is used when no value is present, i.e. when data is missing.  We'll address these values later.\n",
    "\n",
    "As noted earlier, we'd like to extract appraisal value, sale price, and other data for residential real estate.  Based on the documentation, the `PCLASS` field should indicate a parcel's property class.  The first few rows of data are consistent with the documentation.  To see all the values that appear in the `PCLASS` field, we can use the *unique()* method for that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "franklin.PCLASS.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the number of records with each value using the *value_counts()* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "franklin.PCLASS.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentation indicates that the `PROPTYP` also includes property type information; however, the file we have from July, 2015 doesn't even include this field. Let's try to look for a different field which can help us filter our data by the type of property that each row represents.  \n",
    "\n",
    "<hr>\n",
    "\n",
    "<a name=\"Exercise-1\"></a><mark> **Excercise 1** Using `unique()` and `value_counts()` functions in the cells below to confirm that there are only rows containing a \"1.\" or \"nan\" value in the `DWELTYP` field.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "In a future unit, we'll explore how price or appraisal value is dependent on other factors such as number of bathrooms or the year in which a building was built. In order to do this analysis, we'll need to extract the relevant data. \n",
    "\n",
    "Based on the documentation and the first few rows of data, we might be interested in extracting the following columns from the larger dataset.\n",
    "\n",
    "- `APPRLND`: 10 characters numeric representing the total appraised land value for taxable properties\n",
    "- `APPRBLD`: 10 characters numeric representing the total appraised building/improvements value for taxable properties\n",
    "- `LandUse`: 3 character numeric identifier representing the land use code for appraisal purposes\n",
    "- `Cauv`   : 10 character numeric representing the agricultural use value\n",
    "- `SCHOOL` : school district code which is supposed to be a 4 character code, but from our 2015 dataset it appears to be a string\n",
    "- `HOMESTD`: (note the difference from the documentation of \"HOMESTD\" instead of \"HOMSTD\") 1 character indicating whether the property is eligible for a homestead exemption\n",
    "- `TRANDT` : 10 character transfer date of the property (MM/DD/YYYY)\n",
    "- `NAME1`, `NAME2`: Parcel owner's name lines 1 and 2 (25 characters max per field)\n",
    "- `NBRHD`  : 6 character code (although our dataset appears to be comprised of mostly 4 digit codes) identifying the auditor's internal neighborhood code\n",
    "- `PCLASS` : 1 character representing the parcel property class. Valid entries include:\n",
    "        - C >> Commercial property\n",
    "        - E >> Exempt property\n",
    "        - F >> Agricultural property\n",
    "        - I >> Industrial property\n",
    "        - M >> Mineral\n",
    "        - R >> Residential property\n",
    "        - U >> Utility\n",
    "- `PRICE`: 12 characters numeric representing the last known valid sale price\n",
    "- `ACREA`: 10 characters representing an acreage amount\n",
    "- `ROOMS`: 2 characters numeric field representing the total number of rooms.\n",
    "- `BATHS`: 2 characters numeric field representing the number of full baths.\n",
    "- `HBATHS`: 2 characters numeric field representing the number of half baths.\n",
    "- `BEDRMS`: 2 characters numeric field representing the number of bedrooms.\n",
    "- `AIRCOND`: 1 characters representing central air. Valid entries are (0 >> None; 1 >> Heat; >> 2 Heat & Air)\n",
    "- `FIREPLC`: 1 character (Y or N) representing presence of fireplaces\n",
    "- `YEARBLT`: 4 characters representing what year the building was built. Valid entries include:\n",
    "        - 1) any calendar year from 1920 to the present year.\n",
    "        - 2) “OLD” for buildings remodeled before 1920.\n",
    "        - 3) “E 99” where 99 is a valid calendar year for estimated year of remodel\n",
    "- `USPS_CITY`: This undocumented field appears to contain the name of the city used by the US Postal Service (USPS).\n",
    "- `AREA2`: Appears to be a renamed version of the `AREA` field noted in the documentation\n",
    "- `Grade`: Useful fields, but notice the mixed case of the variable name\n",
    "\n",
    "To extract these columns, we'll first create a list containing their names then create a copy of the DataFrame consisting only of the columns. Let's switch over to using our sample dataset, `franklin_auditor_subset.csv` that we saved with the `sample_file` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the subset data file, and assign to variable\n",
    "franklin_subset_file = pd.read_csv('../data/county_auditor/OH-Franklin/franklin_auditor_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# home properties or fields we can use to filter out data\n",
    "franklin_columns = ['ParcelNumber', 'APPRLND', 'APPRBLD', 'LandUse', 'Cauv', 'SCHOOL', 'HOMESTD', 'TRANDT',\n",
    "                    'NAME1', 'NAME2', 'NBRHD', 'PCLASS', 'PRICE', 'ACREA', 'ROOMS', 'BATHS',\n",
    "                    'ANN_TAX', 'DESCR1', 'TAXDESI', 'AREA2', 'DWELTYP', 'COND', 'Grade', 'USPS_CITY', \n",
    "                    'HBATHS', 'BEDRMS', 'AIRCOND', 'FIREPLC', 'YEARBLT', 'WALL']\n",
    "\n",
    "# copy vs view\n",
    "franklin_subset = franklin_subset_file[franklin_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-run our frequency counts\n",
    "franklin_subset.PCLASS.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in `franklin_subset` is a copy of the source data file. We can manipulate the copy while leaving the full dataset unchanged. This can be helpful if we make a mistake or need to see what a value might have been prior to manipulation. Alternatively, we could just reload the data whenever necessary.\n",
    "\n",
    "The first thing we can do is remove data for non-residential properties. As shown above, your version of the sample data file should contain between ~35,000 and 42,000 records which are coded \"R\", a residential property. To filter the data, we can use a mask and bracket notation with the DataFrame. The mask we'll need is one that evaluates to `True` when the value of `PCLASS` is *R*.\n",
    "\n",
    "Once we've filtered the data, we can drop the `PCLASS` column since it is no longer needed. To do this, we'll use the DataFrame's `drop()` method and specify the column name and axis. We'll specify an axis value of `1` to indicate that we'd like to drop a column as opposed to a value of `0` to drop a row. We'll also use the `inplace` keyword argument to indicate that we'd like to manipulate the DataFrame itself rather than to return a DataFrame with the dropped column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the data using a mask\n",
    "franklin_subset = franklin_subset[franklin_subset.PCLASS == 'R']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that that the DataFrame has been filtered by comparing number of records in the `franklin_subset` DataFrame to the number of records in the `franklin_subset_file` DataFrame.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<a name=\"Exercise-2\"></a><mark> **Exercise 2** In the cell below, use the `<` comparison operator to confirm that the number of records in the `franklin_subset.PCLASS` Series is less than the number of records in the `franklin_subset_file.PCLASS` Series, and by extension, the DataFrame.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've confirmed that the data has been filtered, we can drop the `PCLASS` column since it is no longer needed. To do this, we'll use the DataFrame's `drop()` method and specify the column name and axis. We'll specify an axis value of `1` to indicate that we'd like to drop a column as opposed to a value of `0` to drop a row. We'll also use the `inplace` keyword argument to indicate that we'd like to manipulate the DataFrame itself rather than to return a DataFrame with the dropped column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the PCLASS column\n",
    "franklin_subset.drop(['PCLASS'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Let's look at the the appraisal-related fields, `APPRLND` and `APPRBLD`. From above, we know that that data in the `APPRBLD` field is stored as floating point numbers. We can use the `describe()` method to calculate some descriptive statistics for `APPRBLD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "franklin_subset.APPRBLD.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the minimum value is zero.  Let's see how many records have a building appraisal value of zero. Because the data is stored as floating point numbers, we should be aware of the [issues](https://docs.python.org/3/tutorial/floatingpoint.html) related to floating point values.  If we choose to continue working with the data as floating point values, we can use the [NumPy `isclose()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.isclose.html) function to create a mask to compare values to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(franklin_subset[pd.np.isclose(franklin_subset.APPRBLD, 0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to working with floating point values, we can convert a column's datatype to `int` when appropriate.  Here, an integer would represent whole dollar amounts and would be meaningful; if decimals are used to record fractions of a dollar, we won't loose much information.  As a Series, each column has an `astype()` method that can be used to convert the column's type.  The method creates a copy so we have to reassign the DataFrame's column when doing the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "franklin_subset['APPRBLD'] = franklin_subset.APPRBLD.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is stored as integers, we can make comparisons more directly using the standard operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(franklin_subset[franklin_subset.APPRBLD == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter the data to include only the rows where the building appraisal value is greater than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "franklin_subset = franklin_subset[franklin_subset.APPRBLD > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<a name=\"Exercise-3\"></a><mark> **Exercise 3** In the cell below, filter the `franklin_subset` DataFrame to exclude rows that have a `APPRLND` of zero.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine APPRLND as an int\n",
    "franklin_subset['APPRLND'] = franklin_subset.APPRLND.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep those rows where APPRLND is greater than 0\n",
    "franklin_subset = franklin_subset[franklin_subset.APPRLND > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify comparisons and other analysis later, we might choose to combine the data related to number of bathrooms into on column.  Because the data currently distinguishes between full and half baths, we can calculate the total number of bathrooms as the sum of the value of `BATH` and half the value of `HBATHS`.  Note that this has the effect of counting two half-bathrooms as a full bathroom; while two half-bathrooms might effect price differently than a full bathroom, we'll effectively ignore any such effect.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<a name=\"Exercise-4\"></a><mark> **Exercise 4** The data type of the `HBATH` and `HBATHS` should a numeric type (an integer or a floating point value) in order to calculate the combined value directly from the existing values. In the cell below, use the `dtypes` property to confirm that the `HBATH` and `HBATHS` columns have a numeric data type.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Rather than using a for loop and calculating net number of bathrooms for each row, pandas supports element-wise multiplication and addition allowing use to do the following.  For this calculation we will treat missing data in the same was as a zero value.  To do this, we use the `fillna()` (pronounced \"fill N/A\") method for the appropriate column and specify the value we'd like to use in place of missing values - zero, in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "franklin_subset[\"Bathrooms\"] = franklin_subset.BATHS.fillna(0) + 0.5 * franklin_subset.HBATHS.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculates the number of bathrooms as defined above and creates stores each row's value in a new column named `Bathrooms`.  \n",
    "\n",
    "<hr>\n",
    "\n",
    "<a name=\"Exercise-5\"></a><mark> **Exercise 5** We no longer need the `BATHS` or `HBATHS` columns. In the cell below, use the *drop()* method to remove these columns from the `franklin_subset` DataFrame.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Next, let's look at the `Bathrooms` column we created to see what type of frequency of dwellings have different numbers of bathrooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "franklin_subset.Bathrooms.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the `Bathrooms` column has values of \"0.0\". At this point, we might contact the person responsible for maintaining the data for clarification on why residential dwellings can still be listed as having zero bathrooms. However, a quick review of the top 50 or so records of our dataset offers some clues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "franklin_subset.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to to the `FIREPLC` column, we can list the unique values to see that the dataset is inconsistent with the documentation; rather than containing a single 'Y' or 'N' character representing the presence or absence of a fireplace the dataset instead contains integer values of 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "franklin_subset.FIREPLC.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also see that `nan` is among the values in your version of the `franklin_subset` dataset.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<a name=\"Exercise-6\"></a><mark> **Exercise 6** In the cell below, use the `fillna()` property with the `FIREPLC` column to replace missing values with zero. Either reassign the DataFrame's `FIREPLC` column with modified data or specify `inplace=True` as an argument to `fillna()` to alter the column in-place.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first replace nan values\n",
    "\n",
    "# verify that it worked with a call of the unique() function on FIREPLC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a new variable, `Fireplaces`, which will be a boolean variable and contain the values of the `FIREPLC` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new variable 'Fireplace' as a str variable by copying the values from FIREPLC\n",
    "franklin_subset['Fireplaces'] = franklin_subset.FIREPLC.astype(bool)\n",
    "\n",
    "# verify that it worked with a call of the value_counts() function on FIREPLC\n",
    "franklin_subset.Fireplaces.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "At this point we have the following columns and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "franklin_subset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1-ConstructData\"></a>\n",
    "### 3. Construct Data\n",
    "\n",
    "Before moving on to the next dataset, it might be useful to give the columns more descriptive names.  First, let's create a copy of the DataFrame in case we need access to the data in its current state later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy dataframe to a new variable, home_data\n",
    "home_data = franklin_subset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To rename the columns, we can use the DataFrame's `rename()` method. When calling the method, we can pass a dictionary that maps the existing column names to new names. We'll also specify that we want to change column names rather than index labels by specifying `axis=1` and that we'd like to alter the DataFrame itself rather than return a copy with the alteration using `inplace=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data.rename(\n",
    "    {'APPRLND': 'AppraisedTaxableLand',\n",
    "     'APPRBLD': 'AppraisedTaxableBuilding',\n",
    "     'SCHOOL':  'SchoolDistrict',\n",
    "     'HOMESTD': 'HomesteadFlag',\n",
    "     'TRANDT':  'TransferDate',\n",
    "     'NAME1':   'OwnerNameLine1',\n",
    "     'NAME2':   'OwnerNameLine2',\n",
    "     'NBRHD':   'NeighborhoodCode',\n",
    "     'PRICE':   'SalePrice',\n",
    "     'ACREA':   'Acreage', \n",
    "     'AREA2':   'Area',\n",
    "     'ROOMS':   'Rooms',\n",
    "     'ANN_TAX': 'AnnualTaxes',\n",
    "     'Cauv':    'CAUV',\n",
    "     'DWELTYP': 'DwellingType',\n",
    "     'COND':    'Condition',\n",
    "     'BEDRMS':  'Bedrooms',\n",
    "     'AIRCOND': 'AirConditioning',\n",
    "     'YEARBLT': 'YearBuilt',\n",
    "     'WALL':    'WallType'\n",
    "    },\n",
    "    axis=1 ,\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<a name=\"Exercise-7\"></a><mark> **Exercise 7** In the cell below, verify that the columns of the `home_data` DataFrame have been changed.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "As noted above, we'd also like to record whether or not a parcel includes heating.  Earlier we assumed that all residential properties in the data set did include heating.  To add a column with the same value for each row, we can write a statement that assigns that value to the new column in the DataFrame.  Similarly, we'll add a `County` column to indicate the source of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data['Heat'] = True\n",
    "home_data['County'] = \"Franklin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the first few rows to examine the state of our data before moving on to the next dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is also a good time to use an additional feature of Jupyter Notebooks: the Save and Checkpoint option. At the top of the Jupyter Notebook menu bar, choose File -> Save and Checkpoint\n",
    "\n",
    "<img src=\"../images/Jupyter_how_to_image1.jpg\"></img>\n",
    "\n",
    "Doing so will allow you to close the notebook (and Jupyter, if you wish). Clicking on the floppy disk icon just below the 'File' menu option performs the same function. Saving your work in Jupyter Notebooks also saves the result outputs, which can be helpful if you have no need of referencing your prior work, but don't want to re-run all of the code.\n",
    "\n",
    "If you shutdown Jupyter, then you will shut down the kernel running Python in the background. In that case, when you reopen the notebook, Python will not be aware that you have imported modules, loaded datasets, changed variables, and other important work. In order to quickly get back to where you left off without stepping through every cell in the Notebook:\n",
    "- (Optional) Choose the menu option Kernel -> Restart and Clear Output to clear prior code outputs\n",
    "- Click on the last cell in the notebook that you executed\n",
    "- Choose the menu option Cell -> Run All Above\n",
    "\n",
    "<img src=\"../images/Jupyter_how_to_image2.jpg\"></img>\n",
    "\n",
    "Jupyter will re-run all of the preceding code in sequence, as if you were running each line from the command line. You can now continue where you left off!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1-IntegrateData\"></a>\n",
    "### 4. Integrate Data\n",
    "\n",
    "Now we will begin to use the same select, clean, and construct data steps that we performed with the Franklin County data on additional datasets. As we proceed, we will encounter some different data formats and ways that similar data is recorded which will require further work in order to integrate it with our Franklin County data. When integrating data into cohesive wholes in real-world analytical projects it is common to be required to manipulate and massage data to be useful for analysis. Recording the steps taken in the Data Preparation phase is important for providing context in any conclusions or recommendations at the end of the project. One of the benefits of using Jupyter Notebooks to perform our work is that all of that documentation takes place while we work - no more heavy-duty written work outside of our standard work process!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Parsing Errors with Licking County Auditor Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can augment the Franklin County Auditor data with data from the [Licking County Auditor](https://www.lickingcountyohio.us/). The data we'll use was obtained directly from the auditor's site and was not sampled or modified. Let's try loading the data stored in `data/02-licking.txt`. When you run the below `read_csv` function you will receive an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#licking = pd.read_csv(\"../data/02-licking.txt\")\n",
    "licking = pd.read_csv(\"../data/county_auditor/OH-Licking/Parcels.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exception indicates that there was a problem parsing the data; specifically, pandas expected 8 fields but found 23 on line 3. This could be due to pandas incorrectly guessing what the delimiter is. We could use the csv module's `Sniffer` class to detect the delimiter but visual inspection will suffice.\n",
    "\n",
    "In the code below, we'll print the first five lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_number = 0 \n",
    "with open(\"../data/county_auditor/OH-Licking/Parcels.txt\") as infile:\n",
    "    while line_number < 5:\n",
    "        print(infile.readline())\n",
    "        line_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the output, we can see that the delimiter is probably a semicolon rather than a comma. The pandas `read_csv()` method takes a keyword argument, `delimiter`, that will allow us to specify the appropriate value.\n",
    "\n",
    "*Note: You will encounter another error when running the below code:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking = pd.read_csv(\"../data/county_auditor/OH-Licking/Parcels.txt\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exception message indicates that pandas made it farther into the file before encountering an error. On line 11117, pandas expected to find 194 fields based on the previous lines but instead found 184. Let's investigate further.\n",
    "\n",
    "While we could iterate through the file and collect the line or lines that are of interest to use, we can use the `linecache` module to access a specific line within a file. The code below extracts a typical line (one that did not cause a parser error) and the line that causes a problem. After extracting the lines, the code displays their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import linecache\n",
    "typical_line = linecache.getline(\"../data/county_auditor/OH-Licking/Parcels.txt\", 2)\n",
    "error_line = linecache.getline(\"../data/county_auditor/OH-Licking/Parcels.txt\", 11117)\n",
    "\n",
    "display(typical_line)\n",
    "display(error_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the lines in their entirety isn't very revealing.  \n",
    "\n",
    "<hr>\n",
    "\n",
    "<a name=\"Exercise-8\"></a><mark> **Exercise 8** A difference in the number of fields could be caused by a difference in the number of delimiters. In the cell below use the [`count()`](https://docs.python.org/3/library/stdtypes.html#str.count) method with each line to display the number of times the delimiter appears.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "It would be helpful if we could compare each field's values between the two lines. To do this we'll use the String [`split()`](https://docs.python.org/3/library/stdtypes.html#str.split) method to separate each line into a list of field values. In addition to the two lines we already have, we'll retrieve the first line from the data for column names. We can use the itertools module's [`zip_longest`](https://docs.python.org/3/library/itertools.html#itertools.zip_longest) function to combine the list of values extracted from each line for comparison.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "\n",
    "header_line = linecache.getline(\"../data/county_auditor/OH-Licking/Parcels.txt\", 1)\n",
    "\n",
    "header_entries = header_line.split(\";\")\n",
    "typical_entries = typical_line.split(\";\")\n",
    "error_entries = error_line.split(\";\")\n",
    "\n",
    "for entry in zip_longest(header_entries, typical_entries, error_entries):\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start at the end of the file and work our way backwards. We can see that each valid value in the \"error_line\" output has been pushed down one column. As we look upwards through the fields, we will eventually see that this result starts at the `fldTopo` header. The \"typical line\" has no value whereas the \"error line\" has a value that seems related to the value associated with the previous field, `fldLUC`. If we look back to the display of each line's content, we can see that \"430 Resturant\" and \"cafeteria and/or bar\" are separated by a semicolon but should be kept together rather than split apart as different field values; als0 note that \"Restaurant\" is misspelled in the source data.  The source data should use quoting if a delimiter appears as part of a data value or avoid using the delimiter in such a capacity.\n",
    "\n",
    "Now that we know what the problems are, there are a variety of ways to address them. One way is to replace all instances of \"430 Restaurant; cafeteria and/or bar\" in the source text with something that doesn't have a semicolon prior to loading it with pandas. In the code below, we assign the problematic value and its replacement value to variables. After reading the content of the file, we use the `replace()` method to substitute occurrences of the first value with the second. We then load the data into a pandas dataframe. Because the pandas `read_csv()` function is expecting a file or stream, and not a string or bytes, we use the [`StringIO`](https://docs.python.org/3/library/io.html#io.StringIO) class to create a stream from the altered content. We specify \"python\" as the `engine` in the `read_csv()` method to avoid warnings about memory. The Python CSV engine provided by pandas is more feature-complete but is slower than the default C engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "old_value = \"430 Resturant; cafteria and/or bar\"\n",
    "new_value = \"430 Resturant, cafeteria and/or bar\"\n",
    "\n",
    "with open(\"../data/county_auditor/OH-Licking/Parcels.txt\") as infile:\n",
    "    content = infile.read()\n",
    "    \n",
    "content = content.replace(old_value, new_value)\n",
    "    \n",
    "licking = pd.read_csv(io.StringIO(content), delimiter=\";\", engine=\"python\")\n",
    "\n",
    "# replace all the leading 'fld' portion of every column header to simplify outputs\n",
    "licking.columns = licking.columns.str.replace(\"fld\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this was relatively straightforward, there are disadvantages to this method.  The primary disadvantage here is that we iterate through the content of the file several times: first we read all the content, then we iterate through it to find and replace the problematic value, then iterate through it to load it into pandas; usually we only iterate through the file once when loading it into pandas.  While this is fine for relatively small files, we should avoid looping through the entirety of a file whenever possible.\n",
    "\n",
    "An alternative method would be to make use of pandas' support for [regular expressions](https://docs.python.org/3.2/library/re.html) when specifying the delimiter. We can use a [negative look-behind assertion](https://www.regular-expressions.info/lookaround.html) to indicate that a delimiter is any semicolon that isn't immediately preceded by the string \"Resturant\".  We could do this with the following call to *read_csv()*:\n",
    "\n",
    "```python\n",
    "licking = pd.read_csv(\"./data/02-licking.txt\", delimiter=\"(?<!Resturant);\", engine=\"python\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data loaded, let's display the first few lines to get a sense of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the Franklin country dataset, we'd like to filter this dataset for only residential buildings.  Unfortunately, there isn't documentation available to describe the content of each column so we'll have to do our best to infer meaning from the column name and values. Looking at the data above, it looks like `PropertyType` or `Style` might be useful to determine which properties are residential and which are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking.PropertyType.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking.Style.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like most of style values are related to residential-type properties.  At this point, we might decide to choose specific styles to filter on or choose to simply exclude records without style information or those that correspond to a commercial style.  \n",
    "\n",
    "Let's see the styles associated with the `Dwelling` property type.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking[licking.PropertyType == 'Dwelling'].Style.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the data to include only the *Dwelling* property didn't reduce the number of styles.  For this example, we'll filter the data to include only *Single Family*, *MFD Home*, *Tri-Level*, *Duplex*, *Bi-Level*, *Multi-Level*, *Condominum*, *Mobile Home*, *Triplex*, and *4-Level*.  Note that *Condominum* is misspelled in the source data.\n",
    "\n",
    "We can create a list of acceptable style values now that can be used to filter the data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_styles = ['Single Family', 'MFD Home', 'Tri-Level', 'Duplex',\n",
    "                     'Bi-Level', 'Multi-Level', 'Condominum', 'Mobile Home',\n",
    "                     'Triplex', '4-Level']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the other columns we'll need.  We had collected sales price data from the Franklin county dataset.  In this dataset, there are quite a few columns with \"sales\" in the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[column for column in licking.columns if \"sales\" in column.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the sample data above, we will likely be interested in the collection of `SalesPrice` columns to determine the sales price. Let's look at the values of these columns for a small number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking[['SalesPrice1', 'SalesPrice2', 'SalesPrice3', 'SalesPrice4']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have nonzero, zero and `NaN` values.  In addition to these columns, the data also contains `SalesDate` columns. To get a better idea of what the prices represent, let's look at the date columns as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking[['SalesPrice1', 'SalesPrice2', 'SalesPrice3', 'SalesPrice4', \n",
    "         'SalesDate1', 'SalesDate2', 'SalesDate3', 'SalesDate4']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we move from the first price/date column to the second, the second price/date column to the third, and so on, we move backward in time.  It seems reasonable then that `SalesPrice1` represents the most recent sales price and the other columns are used to record historic sales data (if it exists).  We'll use the most recent sales price for our work so we'll only need `SalesPrice1`.\n",
    "\n",
    "Just as with the word \"sale\", there are a number of columns that contain the word \"area\". \n",
    "\n",
    "<hr>\n",
    "\n",
    "<a name=\"Exercise-9\"></a><mark> **Exercise 9** In the cell below, display all the columns with \"area\" in their name.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Here, we'll assume `FinishedLivingArea` contains the data need for area.\n",
    "\n",
    "Next, lets look for bathroom data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[column for column in licking.columns if \"bath\" in column.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have columns corresponding to both full and half bathrooms as before but there is a third column for \"other\".  Let's see what values for this field look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking.OtherBaths.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values themselves don't give a clear idea of what the field represents.  Given the lack of documentation, we'd likely contact the person or group responsible for the data for clarification; for our work here, we'll assume this field corresponds to quarter bathrooms.  \n",
    "\n",
    "Examining the sample data above, we can identify the other columns of interest.  Specifically, we'll extract the following columns from the Licking Country dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_columns = [\"ParcelNo\",\"Owner\",\"Grade\",\"Condition\",\"MarketLand\",\"LUC\",\"SchoolDistrict\",\n",
    "                   \"TaxDistrict\",\"Neighborhood\",\"Subtotal\",\"CAUVTotal\",\"LegalDesc\",\"PropertyType\",\n",
    "                   \"Exterior\",\"MarketImprov\",\"SalesDate1\",\"SalesPrice1\",\"AcreageTotal\",\n",
    "                   \"FinishedLivingArea\",\"Rooms\",\"Bedrooms\",\"FullBaths\",\"HalfBaths\",\"OtherBaths\",\n",
    "                   \"Heating\",\"Cooling\",\"FireplaceOpenings\",\"YearBuilt\",\"MailingAddress5\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a mask to filter the data based on style values.  Rather than compare one value to another as we did when filtering the Franklin Country data, we'll instead check if a values is among a list of values.  To do this, we can us the column's `isin()` method to test a value's membership in a specified list. \n",
    "\n",
    "Below we check if each value in the `Style` column is in the `licking_styles` list we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking.Style.isin(licking_styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this mask in the usual way using bracket notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_subset = licking[licking.Style.isin(licking_styles)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that the filtered data contains only the style values we had wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_subset.Style.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's extract only the columns we want. We use bracket notation again with the list of columns we specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_subset = licking_subset[licking_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the first few rows of `licking_subset` to confirm we've extracted what we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the various bathroom columns into one `Bathroom` column in the same way we combined them for the Franklin County dataset.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<a name=\"Exercise-10\"></a><mark> **Exercise 10** In the cell below, combine the values for full baths, half baths and other baths into one columns named `Bathroooms`. Assume the value in `OtherBaths` is equivalent to a quarter of a full bathroom. Additionally, drop the original bathroom-related columns after computing the values for the new column\n",
    "</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Let's look at heating and cooling data . We extracted two columns from the original dataset `Heating` and `Cooling` that contain heating and cooling data, respectively.  Let's look at the heating data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_subset.Heating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target dataset doesn't differentiate among different data sources - it only indicates whether the property has heating or not.  For the Licking County data, we'd like to associate `False` with `No Heat` and `True` otherwise. We can do this by comparing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_subset.Heating = licking_subset.Heating != \"No Heat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the value counts of the field to confirm that the number of `False` entries corresponds to the previous number of `No Heat` entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_subset.Heating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<a name=\"Exercise-11\"></a><mark> **Exercise 11** In the cell below, replace the values in the `Cooling` column with `True` to indicate that a property has cooling and `False` otherwise.\n",
    "\n",
    "</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "We can assume `FireplaceOpenings` correspond to the number of fireplaces. Let's create a new variable `Fireplaces` as we did with the Franklin County dataset which is a boolean representation of whether a residence has at least one fireplace or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_subset.FireplaceOpenings.fillna(0, inplace=True)\n",
    "licking_subset['Fireplaces'] = licking_subset.FireplaceOpenings.astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should be the last modification needed for the Licking County data.  In order to combine the `franklin_subset` and `licking_subset` DataFrames, we need to make sure they have the same column names.  We'll rename columns in the same way we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_subset.rename(\n",
    "    {'ParcelNo': 'ParcelNumber',\n",
    "     'Owner': 'OwnerName',\n",
    "     'MarketLand': 'AppraisedTaxableLand',\n",
    "     'MarketImprov': 'AppraisedTaxableBuilding',\n",
    "     'Subtotal': 'AnnualTaxes',\n",
    "     'Neighborhood': 'NeighborhoodCode',\n",
    "     'CAUVTotal': 'CAUV',\n",
    "     'LegalDesc': 'LegalDescription',\n",
    "     'PropertyType': 'DwellingType',\n",
    "     'Exterior': 'WallType',\n",
    "     'SalesDate1': 'TransferDate',\n",
    "     'SalesPrice1': 'SalePrice',\n",
    "     'AcreageTotal': 'Acreage',\n",
    "     'FinishedLivingArea': 'Area', \n",
    "     'Heating': 'Heat',\n",
    "     'Cooling': 'AirConditioning',\n",
    "     'MailingAddress5': 'USPSCity',\n",
    "     'LUC': 'LandUse'\n",
    "    },\n",
    "    axis=1 ,\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add a column to the identify the source of this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_subset['County'] = \"Licking\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that the columns in `home_data` and `licking_subset` are the same using the Series [`eq()`](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.eq.html) method. Any differences will have to be corrected before merging the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols1 = pd.Series(home_data.columns.sort_values())\n",
    "cols2 = pd.Series(licking_subset.columns.sort_values())\n",
    "cols1.eq(cols2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! We have a different number of columns between the two data sets. Let's view the column lists in alphabetical order to see the error we've made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(home_data.columns.sort_values())\n",
    "display(licking_subset.columns.sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, we have two different fields for the Owner Name in `home_data` - which comes from Franklin County. We could develop some additional rules for concatenating these two fields, but for the sake of time and brevity in this lesson we'll just drop the `OwnerNameLine2` field, and rename `OwnerNameLine1` to `OwnerName` so that it matches the field structure we have from the Licking County data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename OwnerNameLine1 column to OwnerName along with other column name cleanups in the home_data data set\n",
    "home_data.rename(\n",
    "    {'DESCR1': 'LegalDescription',\n",
    "     'TAXDESI': 'TaxDesignation',\n",
    "     'USPS_CITY': 'USPSCity',\n",
    "     'OwnerNameLine1': 'OwnerName'\n",
    "    },\n",
    "    axis=1 ,\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# drop the OwnerNameLine2 column from home_data\n",
    "home_data.drop(['OwnerNameLine2','HomesteadFlag'], axis=1, inplace=True)\n",
    "\n",
    "# now recheck\n",
    "cols1 = pd.Series(home_data.columns.sort_values())\n",
    "cols2 = pd.Series(licking_subset.columns.sort_values())\n",
    "cols1.eq(cols2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out we haven't been inconsistent with our naming of the count of fireplace openings and the fireplace Y/N flag fields, nor the `TaxDesignation` vs. `TaxDistrict`. Let's clean this up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data.rename({'FIREPLC':'FireplaceOpenings','Fireplaces':'FireplacesFlag'}, axis=1, inplace=True)\n",
    "licking_subset.rename({'Fireplaces':'FireplacesFlag','TaxDistrict':'TaxDesignation'}, axis=1, inplace=True)\n",
    "\n",
    "# now recheck\n",
    "cols1 = pd.Series(home_data.columns.sort_values())\n",
    "cols2 = pd.Series(licking_subset.columns.sort_values())\n",
    "cols1.eq(cols2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! Our dataframes now match in column naming conventions. We can proceed with appending them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Concept-Append\"></a>\n",
    "\n",
    "<hr>\n",
    "\n",
    "#### Concept: Append DataFrames\n",
    "To see how we can combine the DataFrames, let look at an example.  We'll start with two DataFrames, each with columns `A` and `B` and with two rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\n",
    "d2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('BA'))\n",
    "\n",
    "display(d1)\n",
    "display(d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To append the content of one DataFrame to the end of another, we can use the DataFrame `append()` method.  We specify `ignore_index=True` to prevent duplication of index labels. We also pass the parameter `sort=True` to make this teaching example easy to follow, but using this parameter can be computationally expensive and potentially unnecessary on large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.append(d2, ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `append()` method does not modify the original DataFrames directly but instead returns the combined DataFrame.  We can append the `licking_subset` DataFrame to `home_data` and assign the result to `home_data`.\n",
    "\n",
    "*Note: Not using the `sort=True` parameter here saves us a few CPU cycles, but you may receive a warning from pandas that the default behavior will switch to `sort=False` in future versions. You can ignore this warning here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data = home_data.append(licking_subset, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `home_data` now has data for two counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data.County.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are there more entries for Licking County than for Franklin County? Remember, we haven't searched for a 10% sample of entries the way that we did with Franklin County. We need to correct this error before proceeding.\n",
    "\n",
    "We could use a variation of the sampling function we wrote earlier for reading in Franklin County data, but in this case, our data already resides in our `home_data` dataframe. As with most things in Python, there is a simpler way to sample a subset of a data frame without much fuss. We simply use the pandas dataframe methods, [`loc`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html#pandas.DataFrame.loc) and [`iloc`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html#pandas.DataFrame.iloc). These methods allows us to reference a set of rows or column by label (`loc`) or by index (`iloc`). In our case, we'll limit our results to `County`=\"Licking\" and then sample from there, using the random list of index values to select our subset of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licking_subset_sample = home_data.loc[home_data['County'] == \"Licking\"]  # break out Licking County rows from home_data\n",
    "t = len(licking_subset_sample)\n",
    "s = int(0.1 * t)  # fraction of total\n",
    "rows = random.sample(range(licking_subset_sample.index[0], t), s)\n",
    "sample_licking = licking_subset_sample.iloc[rows]\n",
    "sample_licking.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've confirmed we've got a 10% sample of the Licking County data, let's re-build our `home_data` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data = home_data.loc[home_data['County'] == \"Franklin\"]\n",
    "home_data = home_data.append(sample_licking, ignore_index=True)\n",
    "home_data.County.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There, that's better. Let's save our progress by writing to a CSV file, but this time we'll use the pipe '|' separator instead of a comma so that we don't run into any further problems when we want to read in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data.to_csv('../data/unit2_home_data.csv', sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Integrate Data, Continued...\n",
    "\n",
    "We continue our work in integrating datasets by now turning to an unexpected dataset format that still provides plenty of value to our analysis: geo-location information. Although this is but one example of gathering data to integrate with other datasets, think of the possibilities of combining other types of datasets: Twitter real-time messages with geo-location information, geo-location information with transactional purchasing information, or even demographic data with pharmacy counter sales. There are endless permutations of data that a project could explore! The possibilities may be endless, but the problem statement of any good analytical project is not. Remain focused on answering the questions of your project to help guide the data that you gather and how you integrate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"GIS-dataset-extraction\"></a>\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Extraction from a GIS Dataset: Fairfield County Auditor Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final dataset we'll work with is the [Fairfield County Auditor Data](https://www.co.fairfield.oh.us/gis/).  This data is stored as [Geographic Information System (GIS)](https://en.wikipedia.org/wiki/Geographic_information_system) data so loading it won't be as straightforward as reading a text file.  To access the data, we'll use the [GeoPandas](http://geopandas.org/) library, which will load the GIS data into a DataFrame so we can work with it in the same way we manipulated the other datasets. Recall that we installed the library using `conda` (or `pip`) at the beginning of this notebook; with the libary installed, we can import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GIS data we're working with is stored in a [format](http://doc.arcgis.com/en/arcgis-online/reference/shapefiles.htm) specified by [ESRI](https://www.esri.com/en-us/home), the developer of [ArcGIS](https://www.arcgis.com/features/index.html), a popular GIS software product.  Data in this format is stored across several files and can be distributed as a single zip file.  We can load the data from the zip file using GeoPanda's `read_file()` function.  The data we'll be using is stored in `data/02-fairfield-gis.zip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.co.fairfield.oh.us/gis/\n",
    "fairfield = geopandas.read_file(\"zip://../data/county_auditor/OH-Fairfield/fairfield_gis_parcels.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object returned by the `read_file()` method is a [GeoDataFrame](http://geopandas.org/data_structures.html#geodataframe), an extension of the pandas DataFrame with additional functionality. The attributes and methods we've used with other DataFrames are available to use when working with GeoDataFrames. For example, we can see the first few rows in the Fairfield County data using the `head()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most part, this looks like what we'd expect for county auditor data. The last column, however, is something we haven't seen yet.  The `geometry` column contains [data](http://desktop.arcgis.com/en/arcmap/10.3/analyze/arcpy-classes/geometry.htm) used to represent the location and shape of geometric features.  We can use this data to construct plots with map data.  \n",
    "\n",
    "In the code below, we use the [Matplotlib](https://matplotlib.org/) library to create a plot; we'll work with this library again later.  Because the geometric data represents geographic objects on Earth's surface, position information is stored using a [coordinate reference system](http://geopandas.org/projections.html).  For simpler manipulation, the code below converts data to use a reference system that relies on standard latitude and longitude which can be used in masks to filter the data.  Once filtered, the data is plotted.  In the resulting plot of [Lancaster](https://www.google.com/maps/place/Lancaster,+OH+43130/@39.7234464,-82.678719,12z/data=!3m1!4b1!4m5!3m4!1s0x88478a5e4f80f267:0x136dd5d79e3b4de5!8m2!3d39.7136754!4d-82.5993294), we can see features such as roads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lancaster\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (16, 16)  # a square figure size works best in this situation\n",
    "fairfield = fairfield.to_crs({'init': 'epsg:3735'})  # use the result from crs above to determine how to plot the map\n",
    "fairfield.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to the task at hand, let's work on cleaning/filtering the Fairfield County data and merging it with the existing data.  \n",
    "\n",
    "Access to data that was used to construct the GIS dataset is available through a link on the Fairfield County Auditors site. The data is hosted on an external [site](http://downloads.ddti.net/fairfieldoh/) and includes a description of the database structure.  We can load the documentation in the notebook; the description for dwelling data is given on page 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(\"../data/county_auditor/OH-Fairfield/02-fairfield-description.pdf\", 1024, 1280)  # see page 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the first few rows and the documentation, we might be able to filter the data based on the values in the `CLASS` column.  Let's look at its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield.CLASS.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll assume the value `R` represents residential data. We can see that we'll likely need the following columns as well:\n",
    "\n",
    "- `PARID`: Parcel ID (not the same format as Franklin and Licking county Parcel Numbers\n",
    "- `ACRES`: Acreage\n",
    "- Airconditioning: We'll create this column using the `HEAT` column in a moment\n",
    "- `APRLAND`: Appraised land value\n",
    "- `APRBLDG`: Appraised building value\n",
    "- `SFLA`: Area (Total Living Area)\n",
    "- `RMBED`: Bedrooms\n",
    "- `FIXBATH`: Bathrooms\n",
    "- `FIXHALF`: Half-bathrooms\n",
    "- County (we'll add this later)\n",
    "- `LEGAL1`: Legal description\n",
    "- `OWN1`: Owner 1\n",
    "- `OWN2`: Owner 2\n",
    "- `WBFP_O`: 'Fireplaces'\n",
    "- `GRDFACT`: Grade - notice this is different than the documentation\n",
    "- `LUC`: LandUse\n",
    "- `MCITYNAME`: 'USPS_CITY'\n",
    "- `YRBLT`: Year built\n",
    "- `RMTOT`: Rooms\n",
    "- `HEAT`: Heat\n",
    "- `TRANSDT`: Transfer Date\n",
    "- `PRICE`: Sale price\n",
    "- `EXTWALL`: Wall type\n",
    "-  We don't have any of these columns: NeighborhoodCode, ParcelNumber, SchoolDistrict, TAXDESI, TransferDate, HomesteadFlag, DwellingType, CAUV, Condition, AnnualTaxes\n",
    "\n",
    "We can filter the data and extract the columns of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield_columns = ['PARID', 'ACRES', 'APRLAND', 'APRBLDG', 'SFLA', 'RMBED', \n",
    "                     'FIXBATH', 'FIXHALF', 'LEGAL1', 'OWN1', 'OWN2', 'WBFP_O',\n",
    "                     'GRDFACT', 'LUC', 'MCITYNAME', 'YRBLT', 'RMTOT', 'HEAT',\n",
    "                     'PRICE', 'EXTWALL', 'TRANSDT']\n",
    "fairfield_subset = fairfield[fairfield.CLASS == 'R'][fairfield_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look the first few rows of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the `FIXBATH` and `FIXHALF` columns into a single column using the same method we used for the Franklin County data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield_subset['Bathrooms'] = fairfield_subset.FIXBATH + 0.5 * fairfield_subset.FIXHALF\n",
    "fairfield_subset.drop([\"FIXBATH\", \"FIXHALF\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turing to the `HEAT` column, the data documentation indicates that the values in this column represent a \"heat code\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield_subset.HEAT.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the tables in the database available online is one that defines these values.  The heat codes are as follows:\n",
    "\n",
    "- 1: None\n",
    "- 2: Basic\n",
    "- 3: Air conditioning\n",
    "- 4: Heat Pump\n",
    "\n",
    "We'll have to extract both heating and cooling data from this column. Notice that the output of `value_counts()` includes a row count for what appears to be missing data.  Before continuing, let's try to determine why there is a missing value. To begin, let's use the `unique()` method for a better representation of the distinct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield_subset.HEAT.unique() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *None* entry in the variable `HEAT` is ambiguous since the documentation does not indicate that this would be a valid entry. At this point we need to decide if should assume that a missing value means no heat or some other type of heating that doesn't correspond to a code. Let's look at a few rows where `HEAT` is an empty string.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<a name=\"Exercise-12\"></a><mark> **Exercise 12** Using a mask and the `head()` method, display the first five rows of `fairfield_subset` where the `HEAT` column `isna()`.\n",
    "</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "It appears that records where `HEAT` is a null value correspond to parcels with no living area (the variable `SFLA`). Let's investigate how many rows have no living area, and some attributes about those rows using the `describe()` method once again, before modifying our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield_subset[fairfield_subset.HEAT.isna()].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the `ACRES`, `APRLAND`, and `PRICE` variables we can see that these are all fairly large plots of land, without any livable building space, and sold at widely varying price points, although the mean price was 283k dollars with a standard deviation of 1.67M dollars. It's safe to say that these must be farm fields or other plots of land which have been zoned residential, but without any home erected on them. Additionally, there are 9,671 rows in the dataset with no value for `HEAT`, which is about 20% of our dataset. If we did not remove these entries from our data, it would certainly cause problems with accurately determining characteristics about home prices in the central Ohio region. Therefore, we can filter the data again to exclude records with a null value in `HEAT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield_subset = fairfield_subset[fairfield_subset.HEAT.notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves the following values in the `HEAT` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield_subset.HEAT.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we could write code that iterate through the rows of the DataFrame and sets heating and cooling values at the same time, it is easier to split this into two tasks: set the cooling value then set the heating value. \n",
    "\n",
    "We can create a new column, `AirConditioning` based on whether or not `HEAT` has a value of '3'. Because the column contains strings, it's important that our masks compare the column's values to another string rather than an integer, i.e, our mask for air conditioning should be\n",
    "\n",
    "```python\n",
    "fairfield_subset.HEAT == '3'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rather than\n",
    "\n",
    "```python\n",
    "fairfield_subset.HEAT == 3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield_subset['AirConditioning'] = fairfield_subset.HEAT == '3'\n",
    "fairfield_subset.AirConditioning.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can assign a new value to `HEAT` based on the existing value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield_subset.HEAT = fairfield_subset.HEAT != '1'\n",
    "fairfield_subset.HEAT.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final steps are to rename the columns, add data about the source, and append the Fairfield subset to our larger dataset.\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "<a name=\"Exercise-13\"></a><mark> **Exercise 13** In the cell below, rename the columns of the `fairfield_subset` DataFrame so they are consistent with the columns in `home_data`.\n",
    "</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairfield_subset.rename(\n",
    "    {'PARID': 'ParcelNumber',\n",
    "     'ACRES': 'Acreage',\n",
    "     'APRLAND': 'AppraisedTaxableLand',\n",
    "     'APRBLDG': 'AppraisedTaxableBuilding',\n",
    "     'SFLA': 'Area',\n",
    "     'RMBED': 'Bedrooms',\n",
    "     'LEGAL1': 'LegalDescription',\n",
    "     'OWN1': 'OwnerName',\n",
    "     'WBFP_O': 'FireplaceOpenings',\n",
    "     'GRDFACT': 'Grade',\n",
    "     'LUC': 'LandUse',\n",
    "     'MCITYNAME': 'USPSCity',\n",
    "     'YRBLT': 'YearBuilt',\n",
    "     'RMTOT': 'Rooms',\n",
    "     'HEAT': 'Heat',\n",
    "     'PRICE': 'SalePrice',\n",
    "     'EXTWALL': 'WallType',\n",
    "     'TRANSDT': 'TransferDate'\n",
    "    },\n",
    "    axis=1 ,\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we've thus far been looking at all of Fairfield County's data. Before we go further, it would be wise to sample our data now so that we don't have to do so after joining it to the larger dataset of `home_data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "t2 = len(fairfield_subset)\n",
    "s2 = int(0.1 * t2)  # fraction of total\n",
    "rows = random.sample(range(fairfield_subset.index[0], t2), s2)\n",
    "fairfield_subset = fairfield_subset.iloc[rows]\n",
    "fairfield_subset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the `County` column to the dataset, and once again compare the `fairfield_subset` dataframe to the `home_data` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data = pd.read_csv('../data/unit2_home_data.csv', sep='|', encoding='utf-8', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols1 = pd.Series(home_data.columns.sort_values())\n",
    "cols2 = pd.Series(fairfield_subset.columns.sort_values())\n",
    "cols1.eq(cols2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(home_data.columns.sort_values())\n",
    "display(fairfield_subset.columns.sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we didn't have all of the necessary columns in our Fairfield dataset that we had available in Franklin and Licking county datasets. Let's create some placeholder fields in the Fairfield dataset and set their values to nulls (*NaN* to be precise) so that we can cleanly merge `home_data` and `fairfield_subset` together. We'll deal with the null value fields in a programmatic way in a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fairfield_subset['AnnualTaxes'] = np.nan\n",
    "fairfield_subset['CAUV'] = np.nan\n",
    "fairfield_subset['Condition'] = np.nan\n",
    "fairfield_subset['County'] = 'Fairfield'\n",
    "fairfield_subset['DwellingType'] = np.nan\n",
    "fairfield_subset['FireplacesFlag'] = fairfield_subset.FireplaceOpenings > 0\n",
    "fairfield_subset['NeighborhoodCode'] = np.nan\n",
    "fairfield_subset.drop(['OWN2'], axis=1, inplace=True)\n",
    "fairfield_subset['SchoolDistrict'] = np.nan\n",
    "fairfield_subset['TaxDesignation'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Now we can append the `fairfield_subset` dataframe to our `home_data` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data = home_data.append(fairfield_subset, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our dataset contains data from three counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data.County.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often when we work with data, we encounter duplication - repetition of data.  We can see if `home_data` contains duplicate data by comparing the number of rows that would be left if we removed duplicates using the `drop_duplicates()` method to the number of rows in the current DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(home_data.drop_duplicates())/len(home_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<a name=\"Concept-DataManip\"></a>\n",
    "##### Concept: Data Manipulation Resulting in Data Duplication\n",
    "\n",
    "The above equation shows that about 0.0001% of our data corresponds to duplicates. While the original data might not have contained duplicates, we created what appear to be duplicates by removing columns. To see how this happened more clearly, consider the following example DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = d1 = pd.DataFrame([[1, 2, 3], [1, 2, 4]], columns=list('ABC'))\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two rows of data are distinct.  However, if decide we no longer need column `C`, the rows will appear to be duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.drop(['C'], axis=1, inplace=True)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the percentage of rows that would be left after removing duplicates as we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tmp.drop_duplicates())/len(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "While the rows in `home_data` might have corresponded to distinct properties to begin with, at this point there is no way to distinguish between duplicated rows. Be careful with transactional data when this happens; it could be that those duplicate rows were only unique because of some existing errors in how data was recorded. By ignoring the duplication you could be double-counting transactional activity, or inflating statistical counts and averages incorrectly for the purposes of your analyses. In this case, however, we will leave the duplicates in the data knowing that they do represent different properties, not simply transactional data recording errors.\n",
    "\n",
    "Let's look at the data types of our columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `Area` data is stored as objects.  We would probably like to store area as an integer or as a floating point value.  Let's try to convert the area values to integers.\n",
    "\n",
    "*Note: You will receive an error message when attempting to run the next step of code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data.Area = home_data.Area.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exception message indicates that some of the values contain commas. Before continuing, note the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "types = defaultdict(int)\n",
    "for value in home_data.Area:\n",
    "    value_type = type(value)\n",
    "    types[value_type] += 1\n",
    "\n",
    "types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Area` data contains some data stored as integers and other data stored as strings.  The number of strings corresponds to the number of entries from Licking country so the area data was likely stored with commas in that dataset. To resolve this we can iterate through each row and, if the data is a string, remove any commas and convert to an integer. To iterate through the rows of a DataFrame we can use `iterrows()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in home_data.iterrows():\n",
    "    if isinstance(row.Area, str):\n",
    "        new_value = int(float(row.Area.replace(',', '')))  # first remove commas, then cast everything to a float, and finally to int\n",
    "        home_data.loc[index, 'Area'] = new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through the DataFrame row-by-row can be slow and should generally be avoided. An alternative approach is to create a function that would handle one value at a time and use the DataFrame's [`apply()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html) method.\n",
    "\n",
    "Let's look at the data types for each column now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we loaded each data set, we checked some columns for missing values, represented by `NaN` by examining the value counts of those columns. We can check our `home_data` data frame for missing values we might have overlooked . In the code below, we first use the `isna()` method to return `True` for every value that is `NaN` and `False` otherwise.  We then calculate the sum of `True` values for each column using the `sum()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many columns contain missing values. For most of the columns, we might choose to remove any rows that contain missing information; for example, if we plan to use the data to see how different factors affect sales price, we probably don't want to keep records missing price data. Before we start dropping rows, let's address the `NeighborhoodCode` column - there are a significant number of missing values. Recall that when we were working with the Fairfield data, we didn't have any neighborhood codes, so we filled in the column with *NaN* values in order to join to our `home_data` dataset. We can confirm that all the missing `NeighborhoodCode` values correspond to records from Fairfield county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data[home_data.NeighborhoodCode.isna()].County.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing the next unit's learning objectives we may need to filter our data further to remove rows with missing values. Some python modules which produce various charts and graphs won't work with columns containing missing values, and other statistical tools that we will begin learning about it Unit 4 won't produce valid results with missing values. However, for now we are still building our dataset, and can leave these entries as-is.\n",
    "\n",
    "We'll store the data in a SQLite database using SQLAlchemy and the DataFrame's [`to_sql()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html) method; the first argument to this method specifies the table name we'd like to use an the third argument indicates that we would like to replace any existing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///../data/output.sqlite')\n",
    "home_data.to_sql(\"home_data\", con=engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read data back from the SQLite database, we use the `sqlalchemy` method, `read_sql()`. (Now you can easily read/write your data to a simple SQLite database from within Jupyter without having to re-run all prior steps to re-create the same datasets!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///../data/output.sqlite')\n",
    "\n",
    "query = \"SELECT * from home_data;\"\n",
    "home_data = pd.read_sql_query(query, con=engine)\n",
    "display(home_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Related Datasets\n",
    "\n",
    "In the previous examples, we worked on appending the rows of one DataFrame to another. Pandas supports a [variety of methods](https://pandas.pydata.org/pandas-docs/stable/merging.html) of combining data. Another common method is similar to a [database join](https://en.wikipedia.org/wiki/Join_%28SQL%29) where we combine combine the columns of two or more datasets. Pandas DataFrames provide two methods that can be used for data joins: `join()` and `merge()`. The `join()` method can be used when combining datasets based on index values and the `merge()` method can be used to combine datasets on any column values as well as index values; `merge()` is the more general method and `join()` ultimately relies on `merge()` to combine DataFrames. While it is best practice for database administrators to use explicit join conditions to architect high-performing databases, it isn't a necessity to worry about performance issues in data analytics projects. By definition, we are working on a project, not building a system, and therefore are not as concerned about performance or stability of a system. If the `merge()` function is appropriate to use and completes your tasks faster, use it!\n",
    "\n",
    "In the next example, we'll load a new dataset from [Realtor.com](https://www.realtor.com/research/data/). We will then join some of this data to our auditor data file, `home_data` to create a new time-series based dataset. In a future lesson, we'll explore relationships within and between these datasets but let's just focus on how we can combine the datasets first. We begin by downloading the Realtor.com single-family home, county level data from the [Realtor.com](https://www.realtor.com/research/data/) site. *Note: To avoid network issues, we've included the file in your lesson materials: `./data/Realtor_com_Data/RDC_InventoryCoreMetrics_County_sfh.csv`.*\n",
    "\n",
    "<hr>\n",
    "\n",
    "<a name=\"Exercise-14\"></a><mark> **Exercise 14** In the cell below, use the Pandas `read_csv()` function to load the data from `./data/Realtor_com_Data/RDC_InventoryCoreMetrics_County_sfh.csv` and store it in a variable named `realtor_county`. You might encounter a warning message about columns containing mixed types. One solution is to use the more feature-complete Python engine rather than the faster C engine; to do this, add `engine='python'` as an argument to `read_csv()`.\n",
    "</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with most of the datasets we've dealt with in this lesson, we need to select, clean, and construct the dataset before integrating it with our existing `home_data`. Review the following code, run it, and describe what is happening in the Markdown cell, below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(realtor_county.describe())\n",
    "display(realtor_county.columns.tolist())\n",
    "display(realtor_county.head())\n",
    "\n",
    "realtor_columns = ['Month', 'CountyName', 'CountyFIPS', 'Nielsen Rank', 'Median Listing Price', 'Active Listing Count ',\n",
    "    'Days on Market ', 'New Listing Count ', 'Price Increase Count ', 'Price Decrease Count ', 'Pending Listing Count ',\n",
    "    'Avg Listing Price', 'Total Listing Count', 'Pending Ratio']\n",
    "\n",
    "realtor_subset = realtor_county[realtor_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Write your observation of what is happening in the code above, here:</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "As we can see, this new dataset contains monthly information about aggregate home sales as recorded by Realtor.com for each county in the country, and we've been able to extract our three central Ohio counties from this dataset. Let's describe our data now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realtor_subset.CountyName.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realtor_subset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very good. We have 83 months of data per county, and a nice set of aggregate measurements for home sales. Importantly, we now have data such as Nielsen Rank, Days on Market, and more information about listing vs. final sales prices which we did not have available in our auditor data. Now that we see some of the core data available, let's prepare it for joining to our existing `home_data` dataset.\n",
    "\n",
    "We can use the same functionality that we used earlier to eliminate all but our three counties, Fairfield, Franklin, and Licking. We can also eliminate the trailing `\", OH\"` portion of the CountyName variable so that we can join it to our dataset. We'll store the result in a new field called `County`, and drop the `CountyName` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit result set to the counties we're interested in\n",
    "counties_list = {'Fairfield, OH','Franklin, OH','Licking, OH'}\n",
    "realtor_subset = realtor_subset[realtor_subset.CountyName.isin(counties_list)].copy()\n",
    "\n",
    "# clean up the CountyName values so we can store in new column County\n",
    "realtor_subset['County'] = realtor_subset['CountyName'].str.replace(r', OH', '', regex=True)\n",
    "realtor_subset.drop(['CountyName'], axis=1, inplace=True)\n",
    "display(realtor_subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "We can now see that we have fairly good data for time series analysis (minimally), but our `Month` column has a lot of unnecessary time values: `00:00:00`. We can use the `replace()` method to replace the `00:00:00` values with an empty string - effectively removing it. We will then use the `strip()` method to remove any leading or trailing white space.  \n",
    "\n",
    "We could iterate through the rows of the DataFrame using a for-loop; however, this tends to be slow. An alternative method is to use the DataFrame's `apply()` method which allows us to specify a function to [vectorize](https://en.wikipedia.org/wiki/Array_programming) an operation to an entire row or column.\n",
    "\n",
    "In the code below, we first define the function that we would like to apply.  The function is written as though it is applied one row at a time - pandas handles the vectorization for us. We define our function `remove_time` first by specifying what we want to happen in each row. We then invoke the function by using the `apply()` method, specifying `axis=1` which indicates that we would like to apply the function along multiple columns and allow us to access their values. If we had multiple date-time fields like `Month` in our table, this would make cleaning up formats very easy indeed! Since this is a simple dataset, we could have chosen to replace the values in the column `Month` directly, but for example purposes we're learning the benefit of the `apply()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_time(row):\n",
    "    month = row['Month']\n",
    "    # remove timestamp and any leading or trailing white space\n",
    "    return month.replace(\"00:00:00\", \"\").strip()\n",
    "    \n",
    "realtor_subset[\"Month\"] = realtor_subset.apply(remove_time, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realtor_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, now our dataset is cleaned up and ready to be joined. In order to join `home_data` and `realtor_subset`, we'll specify the columns whose values will be compared for matching. To simplify this, it is helpful to use the sames column names, including the same case, in both datasets. For our data, we can convert the columns in the sales data to lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_data_tmp = home_data.rename({col: col.lower() for col in home_data.columns}, axis=1)\n",
    "home_data_tmp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realtor_subset_tmp = realtor_subset.rename({col: col.lower() for col in realtor_subset.columns}, axis=1)\n",
    "realtor_subset_tmp.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we merge, let's get rid of the extra spaces in the column names by replacing \" \" characters with \"_\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realtor_subset_tmp.columns = realtor_subset_tmp.columns.str.replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this dataset will be useful to us in the future, let's save it now to our SQLite database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sqlalchemy import create_engine\n",
    "#engine = create_engine('sqlite:///../data/output.sqlite')\n",
    "realtor_subset_tmp.to_sql(\"realtor_history\", con=engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To merge the data, we can use the `merge()` method of one of the two DataFrames. When using `merge()`, we need to specify the other DataFrame and the columns used for matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save time typing, let's rename this dataset as our \"Point-in-Time DataFrame (pit_df)\"\n",
    "pit_df = home_data_tmp.merge(realtor_subset_tmp, on=[\"county\"])\n",
    "pit_df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it appears we've duplicated data again. Let's examine why.\n",
    "\n",
    "First, notice that the duplicated information from the `home_data` dataset resets at row 83. If we think about our two datasets, what did they contain? The `home_data` dataset contained a single listing of every property in the county; a 1:many relationship. The `realtor_subset` dataset contained multiple months of aggregate data for all homes bought and sold in a given county; a many:many relationship (or in the case of just one county like Franklin County, Ohio, a many:1 relationship). When we merged our datasets by county we therefore found a match in `realtor_subset` 83 times for each property listing because `realtor_subset` contains 83 months of aggregated purchasing information.\n",
    "\n",
    "Second, we should ask what point in time our county auditor datasets were gathered in. It would make little sense to combine December 2018 `realtor_subset` prices with 2010 `home_data` auditor information. Market conditions for housing prices, sales volumes, and consumer behavior between those two years was wildly different!\n",
    "\n",
    "How can we solve this problem? Since we are only interested in showing the possibilities of the `merge` function, let's pick just one month's aggregate numbers out of our `realtor_subset` data. We can then merge that one month of data with our original `home_data` dataset. Doing so will allow us to calculate a variety of metrics and attributes about a given property. We'll find uses for these additional metrics and attributes as we explore model building activities in Units 3 and 4 of this course.\n",
    "\n",
    "First, let's see if we have good data for the month of March, 2019:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realtor_subset_tmp[realtor_subset_tmp.month == '2019-03-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we have good data for all three counties. Let's now merge our datasets using only March, 2019 data from the realtor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pit_df = home_data_tmp.merge(realtor_subset_tmp[realtor_subset_tmp.month == '2019-03-01'], on=[\"county\"])\n",
    "pit_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice our use of object-oriented coding, above. Rather than creating more datasets and variables, we just replaced our original code:\n",
    "\n",
    "```python\n",
    "pit_df = home_data_tmp.merge(realtor_subset_tmp, on=[\"county\"])\n",
    "pit_df.head(100)\n",
    "```\n",
    "\n",
    "with new, inline filtering on the `realtor_subset_tmp` dataframe from our previous step where we verified that we had useful data for March, 2019:\n",
    "\n",
    "```python\n",
    "pit_df = home_data_tmp.merge(realtor_subset_tmp[realtor_subset_tmp.month == '2019-03-01'], on=[\"county\"])\n",
    "pit_df.head(10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the merge was successful.  We can use the DataFrame's *shape* property to see the number of columns and rows it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pit_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this DataFrame to the database use created previously so we can access the data later.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<a name=\"Exercise-15\"></a><mark> **Exercise 15** In the cell below, use the `county_info` DataFrame's `to_sql()` method to save the data in the same database that we stored property information. Use the table name `county_info`.\n",
    "</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another dataset we will use later in this course can be the reverse of a single point-in-time. We could join all of the auditor results to each and every month's data from Realtor.com. To build an accurate time-series dataset we would need to retrieve all of the datasets corresponding to each month's data from each auditor's website. This step is too time consuming for this lesson, and we would find that most of the central Ohio auditor websites do not have good historical data readily available online as of March, 2019. An exception is Franklin County, which we will explore in more detail in a later lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  ```python\n",
    "    franklin.PROPTYP.unique()\n",
    "    ``` \n",
    "\n",
    "    or \n",
    "\n",
    "    ```python\n",
    "    franklin.PROPTYP.value_counts()\n",
    "    ```\n",
    "    \n",
    "    \n",
    "2. ```python\n",
    "   len(franklin_subset) < len(franklin)\n",
    "   ```\n",
    "   \n",
    "   \n",
    "3. ```python\n",
    "   franklin_subset['APPRLND'] = franklin_subset.APPRLND.astype(int)\n",
    "   franklin_subset = franklin_subset[franklin_subset.APPRLND > 0]\n",
    "   ```\n",
    "   \n",
    "   \n",
    "4. ```python\n",
    "   franklin_subset[['BATHS', 'HBATHS']].dtypes\n",
    "   ```\n",
    "   \n",
    "   \n",
    "5. ```python\n",
    "   franklin_subset.drop(['BATHS', \"HBATHS\"], axis=1, inplace=True)\n",
    "   ```\n",
    "   \n",
    "   \n",
    "6. ```python\n",
    "   franklin_subset.FIREPLC = franklin_subset.FIREPLC.fillna(value=0)\n",
    "   ```\n",
    "   \n",
    "   or\n",
    "   \n",
    "   ```python\n",
    "   franklin_subset.FIREPLC.fillna(value=0, inplace=True)\n",
    "   ```\n",
    "   \n",
    "   \n",
    "7. ```python\n",
    "   home_data.columns\n",
    "   ```\n",
    "   \n",
    "   \n",
    "8. ```python\n",
    "   display(typical_line.count(\";\"))\n",
    "   display(error_line.count(\";\"))\n",
    "   ```\n",
    "   \n",
    "   \n",
    "9. ```python\n",
    "   for column in licking.columns:\n",
    "       if \"area\" in column.lower():\n",
    "           display(column)\n",
    "   ```\n",
    "   \n",
    "   \n",
    "10. ```python\n",
    "    licking_subset['Bathrooms'] = (licking_subset.fldFullBaths.fillna(0) + \n",
    "                                   0.5 * licking_subset.fldHalfBaths.fillna(0) + \n",
    "                                   0.25 * licking_subset.fldOtherBaths.fillna(0))\n",
    "   licking_subset.drop([\"fldFullBaths\", \"fldHalfBaths\", \"fldOtherBaths\"], axis=1, inplace=True)\n",
    "    ```\n",
    "    \n",
    "    \n",
    "11. ```python\n",
    "    licking_subset.fldCooling = licking_subset.fldCooling == \"Central\"\n",
    "    ```\n",
    "    \n",
    "    \n",
    "12. ```python\n",
    "    fairfield_subset[fairfield_subset.HEAT == ''].head()\n",
    "    ```\n",
    "    \n",
    "    \n",
    "13. ```python\n",
    "    fairfield_subset.rename(\n",
    "        {'PARID': 'ParcelNumber',\n",
    "         'ACRES': 'Acreage',\n",
    "         'APRLAND': 'AppraisedTaxableLand',\n",
    "         'APRBLDG': 'AppraisedTaxableBuilding',\n",
    "         'SFLA': 'Area',\n",
    "         'RMBED': 'Bedrooms',\n",
    "         'LEGAL1': 'LegalDescription',\n",
    "         'OWN1': 'OwnerName',\n",
    "         'WBFP_O': 'FireplaceOpenings',\n",
    "         'GRDFACT': 'Grade',\n",
    "         'LUC': 'LandUse',\n",
    "         'MCITYNAME': 'USPSCity',\n",
    "         'YRBLT': 'YearBuilt',\n",
    "         'RMTOT': 'Rooms',\n",
    "         'HEAT': 'Heat',\n",
    "         'PRICE': 'SalePrice',\n",
    "         'EXTWALL': 'WallType',\n",
    "         'TRANSDT': 'TransferDate'\n",
    "        },\n",
    "        axis=1 ,\n",
    "        inplace=True\n",
    "    )\n",
    "    ```\n",
    "\n",
    "\n",
    "14. ```python\n",
    "    realtor_county = pd.read_csv(\"../data/Realtor_com_Data/RDC_InventoryCoreMetrics_County_Hist.csv\")\n",
    "    ```\n",
    "    \n",
    "    or \n",
    "    \n",
    "    ```python\n",
    "    realtor_county = pd.read_csv(\"../data/Realtor_com/RDC_InventoryCoreMetrics_County_Hist.csv\", engine='python')\n",
    "    ```\n",
    "\n",
    "\n",
    "15. ```python\n",
    "    #from sqlalchemy import create_engine\n",
    "    #engine = create_engine('sqlite:///../data/output.sqlite')\n",
    "    pit_df.to_sql(\"pit_df\", con=engine, if_exists='replace')\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loading and cleansed the data to some extent, next step might be to being exploring the data. In the next unit, we'll calculate simple, descriptive statistics and create exploratory visualizations using the data we prepared in this this unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources and Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [GeoPandas Documentation](http://geopandas.org/)\n",
    "- [*Data Cleaning: Problems and Current Approaches* by Rahm and Do](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.8661&rep=rep1&type=pdf)\n",
    "- [*Data Mining: Concepts and Techniques* by Han, Pei, and Kamber, Section 3.2: Data Cleansing (Safari Books)](http://proquest.safaribooksonline.com.cscc.ohionet.org/book/databases/data-warehouses/9780123814791/3dot-data-preprocessing/32_data_cleaning?uicode=ohlink)\n",
    "- [*Python Data Science Handbook* by VanderPlas, Chapter 3: Data Manipulation with Pandas](https://jakevdp.github.io/PythonDataScienceHandbook/03.00-introduction-to-pandas.html)\n",
    "- [*Python for Data Analysis* by Wes McKinney, Chapter 7: Data Cleaning and Preparation (Safari Books)](http://proquest.safaribooksonline.com.cscc.ohionet.org/book/programming/python/9781491957653/data-cleaning-and-preparation/data_preparation_html?uicode=ohlink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top](#Top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
